--- 
title: "言語統計処理（春学期）"
author: "寺井雅人（Masato Terai）"
date: "Published:2025-02-03, Last update(JST): `r format(Sys.time(), '%Y-%m-%d %X')`"
site: bookdown::bookdown_site
output: bookdown::html_document
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
link-citations: yes
#github-repo: rstudio/bookdown-demo
---

# About

　この資料は、名古屋大学大学院人文学研究科の学生を対象に開講されている言語統計処理（春学期）で使用する資料です。配布する際は、事前に作成者の寺井雅人まで連絡してください（[researchmap](https://researchmap.jp/terai-research)）。<br>
　この資料は、適宜修正、アップデート行う予定です。ファイルの最終更新日を確認するようにしてください。

## 講義概要

　この授業では，受講者が15回の授業終了時に，以下の知識・能力を身につけていることを目標とする。


1. 統計処理の手続きに関する知識

2. 分析結果を適切に解釈し、説明する能力

3. プログラム言語(R)を用いた統計処理能力

4. 統計理論の数学・概念的知識

<div class="alert alert-info">
  <strong>注</strong> 
  
- 毎回の講義で必ずRStudioがインストールされているパソコンを持参してください。

- 講義内容、講義スケジュールは講義の進度等によって変更する可能性があります。

</div>

## 講義スケジュール
- Week 1：オリエンテーション、R(Studio)のインストールと操作、R Markdown
- Week 2：要約統計量
- Week 3：推測統計学
- Week 4：統計的検定の論理と *t検定*
- Week 5：相関分析
- Week 6：単回帰分析
- Week 7：重回帰分析(1)
- Week 8：重回帰分析(2): コーディングの変更
- Week 9：重回帰分析(3): 交互作用の解釈
- Week 10：一般化線形モデル：ロジスティック回帰分析
- Week 11：階層モデル
- Week 12：効果量と検定力分析
- Week 13：ノンパラメトリック検定
- Week 14：tidyverseパッケージによるデータの加工と可視化
- Week 15：言語研究とオープンサイエンス

## 成績評価について
- 小テスト（20%）、演習課題（40%）、レポート試験（40%）
  - 上記の各評価100点満点に換算し、それらを合計した得点が60点以上を合格とする。A+（100-95）、A（94-80）、B（79-70）、C（69-65）、C-（64-60）、F（59-0）

## 留意事項
- 質問は講義内容に関する質問への回答を優先します。取り組んでいる学位論文やその他プロジェクトへの質問への回答は原則お答えしません。

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Week 1: オリエンテーション、R(Studio)のインストールと操作、R Markdown


```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

## 「統計処理」とは

### 量的研究、質的研究
- 本講義のタイトルにある「統計処理」は、<u>研究の目的などに応じて得られたデータをまとめたり、可視化したり解析する行為</u>を指します。言語に関する研究の手法は大きく分けて以下の2つがある。

  1. 量的研究（ quantitative research ）
    - 数量を用いて分析。一般化が目的。「仮説検証型」。
  2. 質的研究（ qualitative research ）
    - 数量化せずに分析。一般化は目的ではない。「仮説生成型」。

　寺井は量的な研究しか行ってきていないため、本講義では量的な研究を行うことを念頭に統計処理の方法について講義を行う。しかし、それらが質的研究と全く関連がないわけではない（もし質的な研究に関心がある受講生の方がいたら、関連すると思う点を教えてください）。

- 量的研究では数量を扱うため、統計手法を用いて分析を行うことが多い。よって、量的研究を行う場合、統計に関する知識が必要不可欠。

- データ収集の前に分析手法をある程度決めておく必要がある（Garbage in, garbage out. ）。使用する統計手法だけでなく、以下のことも十分に考えておくことが必要。

  - どんなデータが必要？(What)
  - どのようにデータを取ればいい？ (How)
  - どれくらいのデータが必要？ (How many)
  - 誰に対してデータを取る？(Who)

## Rとは

- Rはプログラム言語の一種で、統計解析向けの言語。統計解析以外にもWebアプリを作ったり資料を作成したりすることも可能（この資料もRを使って作成しています）。Rは無料でインストール・使用することができる。

- Rに備わっている機能だけで分析を行うことができるが、世界中のRユーザーが開発した機能（**パッケージ**）を無料でインストールすることができる。これにより、様々な分析を行うことができるが、それらのアップデートが行われるため注意が必要である（アップデートにより以前と同じように動かないということも起こりうるため）。

  - RをiPhone本体、パッケージをXやInstagramなどのアプリとイメージするとよい

- Excelなどコードを書かなくても統計解析を行うことができるソフトウェアは多い。しかし、ソフトウェアは無料のものばかりではなく、また実行したい分析を機能として持っていない可能性がある。近年では生成AIなどによって、プログラミングのハードルが以前よりも低くなったと考えられる。ソフトウェアで実行するよりも、プログラミング言語を駆使して分析を行う方が、統計以外の知識にもつながる。最初は「二階から目薬」のような気持ちになるかもしれないが、根気強く、授業者、友人、先輩、そしてAIさん達に頼りながら取り組んでほしい。


## Rのインストール

- 🔊　Windowsを使用している人で、DocumentsのパスにOne Driveが関係している人は教えてください。将来悪さをする可能性がある。

1. Rの公式サイトにアクセス（ https://www.r-project.org/ ）
2. CRANをクリック
3. Japanのリンクをクリック（Yamagata University）
4. 自分のパソコンのOSを選択
5. "install R for the first time"をクリック。全部NextでOK!

- インストールが完了したら、Rを立ち上げて以下のコードを入力する。Enterを押すと命令が実行される。

```{r}

20 + 1

20 - 1

3 * 4

4 / 2

2 ^ 2

```


## RStudioとは

- Rだけでも分析は可能だが、Rだけの解析は難しい。そこで、よりRでの解析を行いやすくする統合開発環境（RStudio）の中でRを使用する。
- 注意点として、Rのインストールも必ず行う必要がある。
- RStudioはWeb版とローカル版があり、前者はオンライン上で使用するが、後者は各個人のパソコンで使用する。Web版の使用には、サーバーが必要であるため、基本的にローカル版（RStudio Desktop）での使用に慣れる方がよい。

## RStudioのインストール

1. RStudioのサイトへ行く（ https://posit.co/download/rstudio-desktop/ ）
2. All Installers and Tarballsで自分自身のパソコンのOSを選び、ダウンロード

## RStudioの機能
- 4つのペイン：

  - ソース

    - 実行するコードのメモ帳
    
  - 環境・履歴
  
    - 読み込んだデータを表示したりや過去に実行したコマンドを記録できる
    
  - コンソール
  
    - R。ソースペインを実行する場所
    
  - ファイル・作図・パッケージ・ヘルプ
  
    - ワーキングディレクトリにあるファイルを表示したり、作成した図を確認したり、インストールされているパッケージを確認したり、パッケージの使い方を確認する
    
## プロジェクト機能

- プロジェクト = ディレクトリ
  -   ファイルや操作履歴を保存できる
  
::: infobox
ディレクトリ（📁）とは、場所のこと。Rは一か所にしか滞在できない、移動する際も、命令してあげないと自分で勝手に移動してくれない。📁ごとが異なる場所であるため、ファイルを読み込んだりする場合も、Rがいる場所と、ファイルがある場所が異なる場合、Rを移動させるか、ファイルをRがいる箇所まで移動させないと読み込むことはできない。
:::
  
- プロジェクトを作成する利点
  -   研究ごとに分析に必要なファイルをまとめることができる
    
### プロジェクトの作成
- ドキュメントディレクトリに新しいフォルダーを作成
    - **名前は、絶対英数字のみ！（Rが関係しそうな場合、ファイル名、フォルダ名に日本語を使わない方が安心です）**
    
    -   作り方を解説しているサイト（[私たちのR](https://www.jaysong.net/RBook/project.html)）

1. RStudioを開き、右上の[Project:]ボタンをクリック。
2. [New Project]をクリックし、[Existing Directory]から、先ほど作成した名前のディレクトリを選択

- <u>今後、この講義で配布されたファイルや、作成したファイルは全てそのディレクトリ内に入れるようにしてください
- 今後、この講義でRStudioを使用する場合は必ずこのプロジェクトを開いてください</u>


## RStudioのカスタマイズ
- [RStudioの設定変更【見た目】](https://qiita.com/masato-terai/items/4200f3817972fede2dd2)

  - 文字のサイズを大きくしたり、背景の色を変更することが可能
  - その他にも、()の色分けをしてくれたりなど分析の補助になる機能もある

## R Markdown入門

- Rの出力結果を文字や写真、リンクなどと一緒に出力できるもの。

- 左上の紙のマークを押して、R Markdownを開く

- 【パーツ１】YAML（YAML　Ain't Markup Language）ヘッダー：文章全体の体裁や情報を操作する

    -   タイトル、サブタイトル
    -   作成者
    -   作成した日時、更新日時も設定可能
    -   どのような形式で作成するか

::: {.infobox .caution data-latex="{caution}"}
**注意**<br>
・ YAMLヘッダーは、RでもMarkdownでもないプログラム言語で記述
:::

-   【パーツ２】コードチャンク：Rのコードを記述するところ

-   【パーツ３】ドキュメントチャンク：[Markdownと呼ばれるプログラム言語](https://www.jaysong.net/RBook/rmarkdown.html#rmarkdown-intro)で記述するところ

    -   見出し、表、箇条書き、強調、斜体など、Wordのリボン部分にある機能をMarkdownで書く

### Knit🧶を押して出力！
- 初期設定はHTMLファイル出力

## ドキュメントチャンク：Markdown記法

### 覚えるのはマストではない。その都度調べてよく使うものを覚えていく

-   Markdownなら生成AIはほぼ完ぺきに正解を教えてくれる
-   [必要最低限で覚えておくとよい記法](https://qiita.com/tbpgr/items/989c6badefff69377da7)
    -   見出し → これはマスト！
        -   #の数で指定。文字との間を半角あけるのを忘れない。
    -   箇条書き
        -   `*`, `+`, `-`のいずれかを入れる。文字との間を半角あけるのを忘れない。
        -   半角スペースを2つ前（もしくはtab）に入れると、レベル２を作れる。さらに2ついれると、、、
    -   強調
        -   \*で挟むと斜体
        -   \*\*で挟むとBold体
        -   \*\*\*で挟むとどうなるでしょう

## Let's 実践

-   以下の文章をR Markdownを使って再現してください。

::: infobox
## 名古屋飯といえば

## ひつまぶし：*Hitsumabushi*

おすすめは以下のお店です。

- **ひつまぶし花岡**
  - 場所：栄
:::

## 答え合わせ

```{markdown}
# 名古屋飯といえば
## ひつまぶし：*Hitsumabushi*
おすすめは以下のお店です。

- **ひつまぶし花岡**
  - 場所：栄

```

## コードチャンクの挿入

### ショートカットキーが便利：[Ctrl] + [Alt] + [I]（Windows）、[Command] + [Option] + [I]（Mac）

-   このコードの中はR。Rで使う関数などを自由に指定できる
-   以下のチャンク内でないと、動かない = Rの命令として実行してもらえない

\`\`\`{r}\
\`\`\`

```{r}
dat <- c(1, 4, 6)

mean(dat)

```

```{r}
plot(dat)
```


## Let's 実践

-   以下をドキュメントチャンクとコードチャンクを使って再現してください。

::: infobox
## 食費の合計

- 以下は、名古屋旅行で使った食費の合計である。

  - **注!** *hitsu*はひつまぶし、*miso*は味噌カツを表す。

```{r}
hitsu <- 1300 * 2

miso <- 1000 * 2

total <- sum(hitsu, miso)
```
:::

## 答え合わせ

```{markdown}
## 食費の合計
- 以下は、名古屋旅行で使った食費の合計である。

  - **注!** *hitsu*はひつまぶし、*miso*は味噌カツを表す

\```{r}
hitsu <- 1300 * 2

miso <- 1000 * 2

total <- sum(hitsu, miso)
\```

```


## 次週までの課題
### 課題内容
- 小テストの準備。本講義の内容に関する簡単なクイズを行います。配布した資料を見返しておいてください。
- 自己紹介の文をR Markdownで作成し、出力したHTMLファイルを提出してください。以下の2項目を必ず入れてください。クラス内や外部へ公開してもいい情報だけを入れてください！

- 名前、出身、研究科、自分の研究したいことを二文くらいでまとめる。
  - レベル分け、箇条書き、強調（e.g., 下線、太字、イタリック）

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献
- 📚外国語教育ハンドブック
- 💻[RStudioの設定変更【見た目】]
- 📚RユーザーのためのRSTudio［実践］入門
- [私たちのR](https://www.jaysong.net/RBook/project.html)

<!--chapter:end:01-week1.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:019-references.Rmd-->

# Week 2：要約統計量

```{r, include=FALSE}
library(ggplot2)
library(dplyr)
library(patchwork)
```

## 事前の確認

-   この講義のRプロジェクトを開いていますか？

-   英数字で名前を付けた本日の講義のファイルを作成しましたか？

    -   .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1.  記述統計、推測統計の違いおよびデータの尺度について理解できる。
2.  データの要約統計量やグラフの意味が理解でき、またそれらをRで算出・作成ができる。

## 今日の格言

::: infobox
プログラムは思った通りには動かない。書いた通りに動くのだ（unknown）
:::

-   前半は主に説明、後半はRを使用して手を動かしながらやってもらいます。

## 記述統計学と推測統計学

### 記述統計学（descriptive statistics）

-   得たデータを要約する統計手法

    -   **統計量（statistic）**の算出、データの可視化

        -   統計量：データから計算された数値のこと（e.g., 平均値や合計値）

### 推測統計学（inferential statistics）

-   得られたデータから、**母集団**の性質を推測する。 測定、将来の予測、因果関係の推測、現象の説明という主に4つの目的

    -   母集団：想定する集団のこと（日本人の英語力を調べたい = 日本人全員）

-   母集団の一部から集めたデータから、母集団全体に当てはまることを理解する

    -   記述統計ではできない

## データの尺度

-   4種類あり、比例、間隔、順序、名義の順に情報量が少なくなる

    -   上の順序のデータを下の順位のデータに変換することはできるが、逆の変換はできない

### 比率尺度（ratio scale）

-   **様々な統計処理に使える**

-   例：長さ、重さ、時間

-   ゼロが「何もない」ことを表す

-   メモリがすべて等間隔

-   データ同士で四則演算ができる

### 間隔尺度（interval scale）

-   例：温度、テストの得点、偏差値、テストの平均点

-   メモリの間隔は同じ

-   ゼロは「何もない」ことを**表さない**

-   「足す」「引く」は可能。「かける」「わる」はできない。

### 順序尺度（ordinal scale）

-   例：順位、投票のランキング、リカート・スケール（Likert scale）

-   順位のデータ

-   メモリの間隔が一定になる保証はない（= 大小のみを指す）。

-   0位もない

-   四則演算ができない

### 名義尺度（nominal scale）

-   データに任意の数を与えたデータ

    -   例：男性は1、女性は2

-   メモリは等間隔でない

-   ゼロは「何もない」を表さない

-   四則演算ができない

## 「～とみなす」

-   テスト得点は厳密にいうと、間隔尺度とは言えない？

    -   配点が1点と5点は5倍の違いがあるか？
    -   0点は能力が全くないと言えるか？

-   しかしテスト得点を統計分析したい場面が多く、統計分析は間隔尺度以上のデータを必要とする。

-   テスト得点の間隔を等間隔にする努力を怠らないのであれば間隔尺度として「みなそう」ということに現状はなっている（完全に合意されているわけではない）。

    -   本講義ではテスト得点を間隔尺度として「みなす」

## 要約統計量

-   データを取ったらとりあえず要約をする。

    -   そのままのデータ（e.g., 100人分の素点）を人間はなかなか理解できない。

    -   集まったデータを目的合わせて要約をし、分かりやすく提示する。

-   大別して**代表値**と**散布度**があり、前者は大体の値、後者はばらつきを表すデータ

### 各統計量

#### 代表値

```{r, include=FALSE}
# データの作成 (通常データ + 外れ値)
set.seed(123)
data_normal <- rnorm(30, mean = 35, sd = 2)  # 正規分布のデータ
outlier <- c(80,90)  # 外れ値
data_with_outlier <- c(data_normal, outlier)  # 外れ値を追加

df <- data.frame(
  value = data_with_outlier
)

summary_stats <- df %>%
  summarise(
    mean_value = mean(value),
    median_value = median(value)
  )
```

-   平均値（mean）

    -   平均値は沢山ある：算術平均、幾何平均、移動平均

    -   本講義では平均は算術平均を指す

    -   算出方法：すべての数を足し合わせて、データの数で割る

-   中央値（median）

    -   **小さい方から並べて**ちょうど真ん中にある値

        -   奇数の時：1, 2, 3 =\> 中央値は2
        -   偶数の時：1, 2, 3, 4 =\> 中央値は \frac{\left(2+3\right)}{2}= 2.5

::: infobox
平均値は外れ値の影響を受けやすいが、逆に外れ値を考慮しやすい。この二つはことなる基準であるため、平均値と中央値の二つを算出して比較するとよい。

```{r, echo=FALSE}
ggplot(df, aes(y = value, x = 1)) +
  geom_jitter(width = 0.1, alpha = 0.6, color = "blue", size = 3) +  # 個々のデータ点
  geom_hline(aes(yintercept = summary_stats$mean_value, color = "Mean"), linewidth = 1) +  # 平均
  geom_hline(aes(yintercept = summary_stats$median_value, color = "Median"), linewidth = 1) +  # 中央値
  scale_color_manual(values = c("Mean" = "red", "Median" = "green")) + 
  labs(title = "外れ値による平均値と中央値の違い",
       y = "得点",
       color = "統計指標") +
  theme_minimal() +
  coord_flip() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  annotate("text", x = 1, y = max(df$value) - 20, 
           label = paste0("Mean: ", round(summary_stats$mean_value, 1), 
                          "\nMedian: ", round(summary_stats$median_value, 1)), 
           hjust = 0, size = 5, color = "black")  # 右上に平均値と中央値を表示
```
:::

-   四分位（quantile）

    -   小さい方から並べたデータを4分割する

        -   第1四分位：データの25%
        -   第2四分位：データの50%の個所 = 中央値
        -   第3四分位：データの75%

-   最頻値（mode）

    -   得られた数値をまとめ、同じ数字が何回出てくるか（度数: Frequency）を数える。その中で一番数が多い数字のこと。

    -   名義尺度や順序尺度水準のデータはこの指標を使うとよい

    -   例：以下は3人のアンケート結果です。それぞれ3つのポケモンを選んでいます。

| 回答者 | 1位        | 2位        | 3位        |
|--------|------------|------------|------------|
| Aさん  | ピカチュウ | イーブイ   | ゲンガー   |
| Bさん  | ピカチュウ | リザードン | ミュウ     |
| Cさん  | ピカチュウ | カビゴン   | フシギバナ |

| ポケモン名 | 得票数 |
|------------|--------|
| ピカチュウ | 3      |
| イーブイ   | 1      |
| ゲンガー   | 1      |
| リザードン | 1      |
| ミュウ     | 1      |
| カビゴン   | 1      |
| フシギバナ | 1      |

-   最頻値は3でピカチュウの得票数が多かった。

#### 散布度

-   分散（variance）：*σ*<sup>2</sup>

    -   データが平均値の付近に密集している程度
    -   値が大きいほどばらつきが大きい

$$\sigma^2=\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2$$

-   *N* : データの数
-   *x*<sub>i</sub> : データ一つ一つ
-   *u* : 平均値

1.  各データを平均値から引き、「平均値との差」という指標に変換（ = 平均からの**偏差**）
2.  全部足したいが、プラスとマイナスが混じっており、全部足すと0になる。
3.  2乗し全部プラスにする。
4.  全部の偏差を足して、データの数で割る。

-   標準偏差（standard deviation）：*σ*

    -   分散のルートを取った値。分散は元のデータを2乗している。そのため単位が大きく解釈が困難なため。
    -   値が大きいほどばらつきが大きい

$$\sqrt{σ^{2}}$$

::: infobox
平均、標準偏差などの要約だけではデータの全体像は見えにくい。下記の図のように、同じ平均、標準偏差（*M* = 50, *SD* = 10）でもデータの分布が異なることが分かる。このように、得られたデータの把握には、数値の要約だけでなく、可視化も重要となる。

```{r, echo=FALSE, warning = F}
# データ1（標準正規分布）
data1 <- rnorm(1000, mean = 50, sd = 10)

# データ2（右に裾が長い分布）
data2_left <- rnorm(850, mean = 50, sd = 8)   # 中心部のデータ（分散調整）
data2_right <- rnorm(150, mean = 70, sd = 20) # 右側の裾を伸ばすデータ

# 2つのデータを結合
data2 <- c(data2_left, data2_right)

# 分散調整: 分散を100にするため、分散が違う場合はスケーリング
scale_factor <- sqrt(100 / var(data2))
data2 <- (data2 - mean(data2)) * scale_factor + 50  # 平均を50に調整

# データフレーム作成
df1 <- data.frame(value = data1, group = "データ1: 正規分布")
df2 <- data.frame(value = data2, group = "データ2: 右に裾が長い分布")

# プロット1（標準的な正規分布）
p1 <- ggplot(df1, aes(x = value)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "通常の正規分布", x = "値", y = "頻度") + xlim(20, 100)

# プロット2（右に裾が長い分布）
p2 <- ggplot(df2, aes(x = value)) +
  geom_histogram(binwidth = 5, fill = "salmon", color = "black", alpha = 0.7) +
  labs(title = "右に裾が長い分布", x = "値", y = "頻度") + xlim(20, 100)

p1 + p2
```
:::

## データの可視化

```{r, echo=FALSE}
set.seed(123)
# 英語の得点（平均 70, 標準偏差 15）
english_scores <- round(rnorm(100, mean = 60, sd = 15))
english_scores <- pmin(pmax(english_scores, 0), 100)  # 0〜100の範囲に収める

# 国語の得点（平均 75, 標準偏差 12）
japanese_scores <- round(rnorm(100, mean = 80, sd = 12))
japanese_scores <- pmin(pmax(japanese_scores, 0), 100)  # 0〜100の範囲に収める

# データフレームを作成
score_data <- data.frame(
  ID = 1:100,  # 生徒ID
  English = english_scores,
  Japanese = japanese_scores
)
```

### 量的データの可視化

#### 1つのデータ

##### ヒストグラム（histogram）

-   データを階級幅で区切り、その中に入るデータがいくつあるか（度数）を描画する

```{r}
score_data %>%
  ggplot(aes(x = score_data$English)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "英語の得点分布", x = "得点", y = "人数") 
```

#### 2つのデータ

##### 散布図（scatter plot）

-   量的データ同士の関係を描画する

    -   以下の図では100人がそれぞれ国語と英語のテストを受け、その得点を描画している。

```{r}
ggplot(score_data, aes(x = English, y = Japanese)) +
  geom_point(color = "blue", alpha = 0.6) +
  labs(title = "英語と国語の得点の関係", x = "英語の得点", y = "国語の得点") +
  theme_minimal()
```

### 量的 + 質的データの可視化

-   科目（名義尺度）ごとの得点（間隔尺度）のような場合。

#### 棒グラフ（bar plot）

-   それぞれの科目の得点の平均値を描画

```{r, echo=FALSE}

score_data2 <- data.frame(
  Subject = c("英語", "国語"),
  Score = c(60, 90)  # 例えば、英語85点、国語90点
)

ggplot(score_data2, aes(x = Subject, y = Score, fill = Subject)) +
  geom_bar(stat = "identity", show.legend = FALSE, width = 0.5) +
  labs(title = "英語と国語の得点", x = "科目", y = "得点") +
  ylim(0, 100) + 
  scale_fill_manual(values = c("英語" = "skyblue", "国語" = "salmon")) 

```

#### 箱ひげ図（boxplot）

-   棒グラフよりもより多くの情報を確認できる。

-   外れ値が含まれる場合、描画されている最大値もしくは最小値はデータの中のもっとも大きい数をではない場合がある。

    -   一般的に外れ値として判定される数

        -   第1四分位数 - 1.5 × IQR（Inter-Quartile Range：第3四分位数から第1四分位数の範囲）
        -   第3四分位数 + 1.5 × IQR

```{r, echo=FALSE}
ggplot(score_data, aes(x = "英語", y = English, fill = "英語")) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 2) +
  geom_boxplot(aes(x = "国語", y = Japanese, fill = "国語")) +
  scale_fill_manual(values = c("英語" = "skyblue", "国語" = "salmon")) +
  labs(title = "英語と国語の得点分布", x = "科目", y = "得点") +
  annotate("text", x = 1.55, y = quantile(score_data$English, prob = 0.75), label = "第3四分位数", color = "blue", size = 4) +
  annotate("text", x = 1.55, y = quantile(score_data$English, prob = 0.25), label = "第1四分位数", color = "blue", size = 4) +
  annotate("text", x = 1.3, y = min(score_data$English), label = "外れ値", color = "red", size = 4) + 
  stat_boxplot(geom='errorbar', width = 0.05) +
  annotate("text", x = 1.15, y = min(score_data$English), label = "最小値", color = "blue", size = 4) +
  annotate("text", x = 1.15, y = max(score_data$English), label = "最大値", color = "blue", size = 4) +
  annotate("text", x = 1.55, y = median(score_data$English), label = "中央値", color = "blue", size = 4) 
```

#### ヴァイオリン・プロット（Violin plot）

-   ヒストグラムを滑らかな曲線（カーネル密度推定）に変え、背中合わせに張り付けたグラフ

    -   データが集まっている個所が膨らんで描画されるため、全体的な分布を把握しやすい

```{r}
ggplot(score_data, aes(x = "英語", y = English, fill = "英語")) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_violin(aes(x = "国語", y = Japanese, fill = "国語"), trim = FALSE, alpha = 0.6) +
  scale_fill_manual(values = c("英語" = "skyblue", "国語" = "salmon")) +
  labs(title = "英語と国語の得点分布（ヴァイオリン・プロット）", x = "科目", y = "得点") +
  theme_minimal()

```

### 質的データの可視化

-   四則演算ができないため、平均値などを算出できない。

```{r, echo=FALSE}
survey_data <- data.frame(
  Pokemon = sample(c("ピカチュウ", "イーブイ"), 100, replace = TRUE),
  Like = sample(c("Yes", "No"), 100, replace = TRUE, prob = c(0.7, 0.3))  # 7割が好き
)
```

-   度数分布表（frequency distribution table）

    -   1次元

```{r, echo = FALSE}
survey_data %>%
  group_by(Like) %>%
  count() %>%
  arrange(desc(Like)) %>%
  tidyr::pivot_wider(names_from = Like, values_from = n) %>%
  kableExtra::kbl(align = "c", caption = "Do you like Pikachu and Eevee?") %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped"),
                full_width = T)
```

-   クロス集計表（cross table）

    -   2次元

```{r}
survey_data %>%
  group_by(Pokemon, Like) %>%
  count() %>%
  arrange(desc(Like)) %>%
  tidyr::pivot_wider(names_from = Like, values_from = n) %>%
  kableExtra::kbl(align = "c", caption = "Do you like Pikachu and Eevee?") %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped"),
                full_width = T) 
```

## ハンズオンセッション

### Rの基本用語

#### 変数

-   どんなデータも格納できる箱。箱には必ず名前を付ける。**名前は英数字のみを入れること。**また、命名には規則がある。

    -   名前の先頭に数字（e.g., 1hako）や記号（e.g., %hako）は使用できない
    -   大文字と小文字は区別される（Hakoとhakoは別の変数として認識される）

::: infobox
プログラミングの世界では、以下の二つの命名の流儀がある。

-   スネークケース🐍：変数の区切りを"\_"で示す（e.g., snake_case）
-   キャメルケース🐫：変数の区切りを大文字で示す（e.g., camelCase）

自由に箱（変数）に名前を付けてよいが、できるだけ自分の中で一貫したルールを持ち、第三者が見ても何が格納されているか分かりやすい名前を付けることを意識する。
:::

-   変数に入れたデータは、［Environment］タブに表示される。

-   変数の中身が数値の場合、数値を足したりかけたりなど計算ができる。

    -   numという名前の変数に1から10の数字を入れ、それらを2倍した

```{r}
num <- 1:10

num

num * 2
```

-   変数の中身は上書きすることもできる

```{r}
num <- num * 2
```

#### データの型

-   「いちたすに」と「1 + 2」はコンピュータにとっては別もの

    -   実数型（numeric）：数値全般（e.g., 10.4）
    -   整数型（integer）：整数のみ（e.g., 10）
    -   文字列型（character）：文字（e.g., "十"）。入力する際は`""`や`''`で囲む
    -   論理型（logical）：TRUEとFALSEからなる
    -   因子型（factor）：データに順番（数字を割り振る）

```{r}
dat <- 1:4

str(dat)

dat <- factor(dat, levels = c(4:1))

str(dat)

```

#### 関数

-   パッケージに含まれている、命令を実行するのに必要なもの。`data.frame()`のような文字列とかっこの組み合わせ。かっこの中にデータを格納する。

    -   数学の関数とイメージしてもよい。例えば、Y = 2x という関数は、xを2倍する関数。入れたデータが2倍されて返ってくる。

    -   使う際は呼び出す必要があり、Rのセッションが切れるまでは何回も呼び出す必要はない。Rに標準で備わっているものは呼び出す必要はない（base関数という）。

        -   下の例では、`plot()`関数で変数の中身を描画している

```{r}

dat.2 <- 1:10

plot(dat.2)
```

-   `help(関数名)`やウェブサイトで調べると、使い方を確認することができる。

#### ベクトル

-   データのまとまり

    -   `c()`関数で作成可能。連続する数値であれば、`:`でもOK

```{r}
numVector1 <- c(1, 35, 90, 0.9)
numVector2 <- c(1, 35, 90, 9)
chVector <- c("いちご", "strawberry", "イチゴ")
numchVector <- c("イチゴ", 1000, "みかん", "500")
```

-   変数に何かを入れたら、必ず中身を確認する！

```{r}
numVector1
numVector2
chVector
numchVector
```

-   データの型がどうなっているかを確認。確認する関数は様々なものがある。[Environment]タブでも表示されている。

```{r}
class(numVector1)
str(numVector2)
typeof(chVector)
mode(numchVector)
```

### データを読み込む

-   研究では、ファイルに格納されたデータに対して分析を行う。データ基本的にxlsx、csv、txtという拡張子のファイルに格納されていることが多い（最近ではオンラインでのデータ収集によってJSON形式もよく見るようになりました）。

    -   xlsx

        -   エクセルファイル。

    -   csv (Comma-Separated Values)

        -   カンマ（,）で項目を区切ったファイル

    -   txt

        -   文字データだけが含まれるファイル（区切りは様々）

-   拡張子を表示させる設定に変更する。

-   データを読み込む場合、ファイルの種類によって読み込む際の関数が異なる。リンク先から（[データ格納庫](リンクを挿入)）データをダウンロードし、本講義用のR project内に移動させる。

-   移動させたら、以下のコードを走らせ、データをR(Studio)に読み込む

    -   文字化けする場合、以下のコードを引数の中に加える（Windows: `挿入`, MAC: `挿入`）。

-   以下の処理で、csvファイル内のデータを`dat`という変数の中に格納したことになる。

```{markdown}
dat <- read.csv()
```

### データフレーム

-   行（横）と列（縦）からなる、ベクトルのかたまり

    -   数値、文字、因子など様々なベクトルを格納するデータ。データ分析のデータは基本的にこの型を指す。

-   読み込んだデータは基本的にデータフレームであるが、ベクトルから`data.frame()`関数で作成することもできる。

```{r}
fruit_df <- data.frame(
  name = c("イチゴ", "みかん"),
  price = c(100, 150)
)
```

-   数値は行の番号を指し、データには含まれない。

```{r}
fruit_df
```

### 要約統計量の算出

#### 各統計量ごと

```{r}
# 合計 (sum)
sum(mtcars$mpg)

# 平均 (mean)
mean(mtcars$mpg)

# 中央値 (median)
median(mtcars$mpg)

# 最大値 (max)
max(mtcars$mpg)

# 最小値 (min)
min(mtcars$mpg)

# 標本分散 (var)
var(mtcars$mpg)

# 標本標準偏差 (sd)
sd(mtcars$mpg)

# （最大 - 最小）
range_mpg <- range(mtcars$mpg)
wide_mpg <- diff(range_mpg)
wide_mpg  # 範囲を表示

# 尖度 (kurtosis) ※"moments" パッケージを使用
#install.packages("moments")  # 初回のみ
library(moments)
kurtosis(mtcars$mpg)

# 四分位範囲 (IQR)
IQR(mtcars$mpg)
```

#### 一度に出力

-   `base`関数

```{r}
base::summary(mtcars$mpg)
```

-   `psych`パッケージの`describe`関数

```{r}
# install.packages("psych") 1つのパソコンにつき一回でよい
library(psych) # 使用する際、セッションにつき一回
describe(mtcars$mpg)
```

#### 最頻値

-   Rでは最頻値の求め方に少し一工夫必要。いろいろなやり方があるが、その一例として下記がある。

```{r}
# 最頻値 (mode) ※最頻値は複数ある場合があるため table() を使用
table_mpg <- table(mtcars$mpg)
mode_mpg <- as.numeric(names(table_mpg)[table_mpg == max(table_mpg)])
mode_mpg  # 最も頻度の高い値を表示

```

### 作図

-   Rに最初から登録されているデータセットを使用

    -   [参考](https://www.geeksforgeeks.org/a-complete-guide-to-the-built-in-datasets-in-r/)

-   描画のパラメータ。詳しくはウェブサイトを参考に（[例](https://eau.uijin.com/advgraphs/parameters.html) ）

    -   main: 図のタイトル
    -   xlab: x軸のタイトル
    -   ylab: y軸のタイトル
    -   border: ヒストグラムの棒の枠線
    -   type: 線や点のスタイル
    -   col: 色を指定（e.g., "blue", "red", "green"）
    -   pch: 点の形指定（e.g., pch = 16 は●、pch = 17 は▲）
    -   lwd: 線の太さを指定

#### ヒストグラム

```{r}
hist(mtcars$mpg, 
     main = "Histogram of Miles Per Gallon (mpg)", 
     xlab = "Miles Per Gallon", 
     col = "lightblue", 
     border = "black")

```

#### 折れ線グラフ

```{r}
plot(airquality$Temp, 
     type = "l", 
     main = "Daily Temperature in New York", 
     xlab = "Day", 
     ylab = "Temperature (°F)", 
     col = "blue", 
     lwd = 2)

```

#### 散布図

```{r}
plot(mtcars$wt, mtcars$mpg, 
     main = "Scatter Plot of Car Weight vs. MPG", 
     xlab = "Weight (1000 lbs)", 
     ylab = "Miles Per Gallon", 
     col = "darkgreen", 
     pch = 16)

```

#### 箱ひげ図

```{r}
boxplot(mtcars$mpg[mtcars$vs == 1], mtcars$mpg[mtcars$vs == 0], 
        names = c("Engine: straight", "Engine: V-shaped"),
        main = "Boxplots of Miles Per Gallon", 
        ylab = "Values", 
        col = c("lightcoral", "lightblue"))
```

#### バイオリンプロット

```{r}
#install.packages("vioplot")

library(vioplot)

vioplot(mtcars$mpg[mtcars$vs == 1], mtcars$mpg[mtcars$vs == 0], 
        drawRect = F, # Tにすると、箱ひげ図が中に描かれる
        names = c("Engine: V-shaped", "Engine: straight"), 
        col = c("lightcoral", "lightblue"), 
        main = "Violin Plot of Miles Per Gallon", 
        ylab = "Miles Per Gallon")
```

## まとめ

（引用：平井 et al. (2021). 『教育・心理系研究のためのRによるデータ分析』 p. 3）

| 種類         | 指標                            | 特徴                                                                                                                                                                                                                                                                  |
|----------------|----------------|----------------------------------------|
| 名義尺度以上 | 最頻値 (mode)                   | 最も多い度数を示すデータの値。主に名義尺度で用いられる代表値。                                                                                                                                                                                                        |
| 順序尺度以上 | 中央値 (median)                 | データを順番に並べたときの真ん中（50%タイル）の値。順序情報に基づくため外れ値の影響を受けにくい。<br>（例）テストの得点が {1, 3, 5, 7, 9} の場合は、中央値は 5 になる。                                                                                               |
| 間隔尺度以上 | 平均 (mean)                     | 個々の測定値の和を測定値の個数で割った値。中央値に比べ、外れ値に引っ張られる傾向がある。なお、標本平均 $\bar{x}$ と区別して母集団の平均を表す場合は平均（$\mu$）と呼ぶ。                                                                                              |
| 名義尺度以上 | 平均情報量                      | 総度数と各カテゴリ度数との比率。<br>（例）本の貸し出し総数が 10 件とすると、総度数は 10 件。そのうち、フィクションは 3 件、実務書 3 件、ノンフィクション 2 件、それ以外のジャンルは 2 件とカテゴリ度数を示す。                                                        |
| 順序尺度以上 | 範囲 (range)                    | 最大値と最小値との差。                                                                                                                                                                                                                                                |
| 順序尺度以上 | 四分位偏差 (quartile deviation) | 順に並んだデータを 4 等分し、その境界となる第1四分位数（$Q_1$：25%タイル）と第3四分位数（$Q_3$：75%タイル）の差を四分位範囲（inter quartile range）と言う。それを 2 で割った値が四分位偏差。<br>$Q = \frac{Q_3 - Q_1}{2}$                                             |
| 間隔尺度以上 | 分散 (variance)                 | 平均からの偏差平方和の大きさを示す。データ $x_i$ と平均（$\bar{x}$）の差を2乗して、全データ（$n$）またはデータ数 $n-1$（標本分散）で割った値。統計では、不偏不分散（unbiased variance）として、$n-1$ で割ることが多い。<br>$s^2 = \frac{\sum (x_i - \bar{x})^2}{n-1}$ |
| 間隔尺度以上 | 標準偏差 (standard deviation)   | 上記の分散の平方根を取った値で、非負値になる。単位が元の値と同じ尺度なので直感的に解釈しやすい。小さい値はデータが平均の近くにあり、個別データのばらつきが小さいことを示す。<br>$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n-1}}$                                       |

## 次週までの課題

### 課題内容

1.  小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2.  下記のデータをもとに、どの指標でもよいので、以下の内容を含めること。R Markdownファイルで作成し、HTMLファイルに変換しそれを提出

-   要約統計量を算出

-   最低2つの図

-   気づいたことを2行以上でまとめる

-   ポケモンのデータセット（[入手元](https://smart-hint.com/poke-data/introduction/#%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%82%92%E5%AD%A6%E3%81%B6)）

    -   一部列を削除している。

-   データの内訳

    -   id: 各ポケモンに振られた数値
    -   名前：ポケモンの名前
    -   タイプ1：ポケモンに設定されている属性のようなもの。複数のタイプを持つポケモンがいるので、タイプ1となっている
    -   高さ：身長
    -   重さ：体重
    -   世代：ポケモンには1\~8まで世代が存在し、新しいゲームが出るたびに新種のポケモンが発表される
    -   ステータス：ポケモンが持つ6つ能力（HP、こうげき、ぼうぎょ、とくこう、とくぼう、すばやさ）の合算値
        -   HP: Hit Pointで体力のこと
        -   こうげき：物理技の攻撃力
        -   ぼうぎょ：物理技の防御力
        -   とくこう：特殊技の攻撃力
        -   とくぼう：特殊技の防御力
        -   すばやさ：どれくらい速くワザを出せるか
    -   捕まえやすさ：数値が上がるほど捕まえやすくなり最大で255で、最小は3
    -   進化：0 = 進化しないポケモン、1 = たねポケモン、2 = 1進化したポケモン、3 = 2進化したポケモン
    -   画像URL：コピー&ペーストすればどんなポケモンか見れる

### 提出方法

-   メールにファイルを添付して送信。
-   締め切りは今週の木曜日まで

## 参考文献

-   外国語教育ハンドブック
-   Rでらくらくデータ分析
-   心理学統計法　放送大学
-   <https://www.geeksforgeeks.org/a-complete-guide-to-the-built-in-datasets-in-r/>
-   <https://eau.uijin.com/advgraphs/parameters.html>
-   <https://smart-hint.com/poke-data/introduction/#%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%82%92%E5%AD%A6%E3%81%B6>

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:02-week2.Rmd-->

# 推測統計学

```{r, include=FALSE}
library(tidyverse)
library(patchwork)
library(gridExtra)
library(gganimate)
```

## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 母集団と標本の違いや確率分布が理解できる
2. 区間推定の考え方が理解できる

## 母集団と標本
- 推測統計学は、**母集団**から**標本**を得て、母集団の特徴を推測する手法

```{r, echo=FALSE}
# 母集団データ（正規分布を仮定）
set.seed(123)
population <- data.frame(value = rnorm(10000, mean = 50, sd = 10))

# 標本を抽出
sample_data <- as.data.frame(population[sample(1:10000, 100), ])
colnames(sample_data) <- "value"

# 可視化
ggplot() +
  geom_histogram(data = population, aes(x = value), bins = 30, fill = "grey", alpha = 0.5) +
  geom_histogram(data = sample_data, aes(x = value), bins = 30, fill = "blue", alpha = 0.5) +
  labs(title = "母集団と標本の概念図（100点満点のテストの例）", x = "値", y = "頻度",
       subtitle = "灰色：母集団、青色：標本") +
  theme_minimal() + 
  theme(axis.text.y = element_blank())
```

### 母集団（population）

- 調べたい対象の集団のこと
  
  - 例

    - 研究課題：英語を第二言語として学ぶ人は母語の影響を必ず受ける。
    
    - 母集団：世界中の英語を第二言語として学ぶ人

### 標本（Sample）
- 実験や調査のために母集団の中から選ばれたもの

  - 選ぶことを標本抽出（sampling）と呼ぶ

## 確率と確率変数
- 推測統計学では確率が非常に重要な概念。

### 確率

### 確率変数と確率分布

#### 確率変数（random variable）
- 確率的に生じる事象に数値を割り当てたもの

  - 実際に得られた値のことを確率変数の**実現値**という
  
- 確率変数はスロットマシーンが回っている状態。実現値はスロットマシーンが止まって値が定まった状態をイメージするとよい

```{r, include = F}
# アニメーション用データを作成
set.seed(123)
df <- expand.grid(frame = 1:7, slot = factor(1:3)) %>%
  mutate(value = sample(1:7, n(), replace = TRUE))  # 1-7のランダム値

# スロットマシンの回転アニメーション
p<- ggplot(df, aes(x = slot, y = 1, label = value)) +
  geom_text(size = 15, fontface = "bold", color = "skyblue") +
  ylim(0.5, 1.5) +
  theme_minimal() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  ggtitle("スロットマシン") +
  transition_states(frame, transition_length = 1, state_length = 0.5) +
  enter_fade() + exit_fade()

p
```

#### 確率分布（probability distribution）
- 確率変数の実現値それぞれの生じやすさを確率で表したもの。統計学では、確率変数の実現値を与えると確率を返す関数のことを確率分布という

- 事象を確率変数として扱うことで、母集団を人の集団ではなく、確率変数の実現値の集合として考えることができる。

  - 1万人の学生の集団という母集団が、数値が1万個ある集団と仮定でき、数学的な枠組みとして考えやすくなる（数学の世界に現実世界を映す）。

- この場合、母集団の性質を標本に正確に反映させるために、**単純無作為抽出（simple random sampling）**を行う必要がある。

  - 母集団のすべての対象が偏りなく選ばれること（= 無作為に選ぶ）

#### 確率分布の種類
- 離散型確率分布：確率質量関数
  - とびとびの値をとる（e.g., 裏、表）
  - ベルヌーイ分布、二項分布、ポアソン分布

```{r, echo=FALSE}
# データを作成
# ベルヌーイ分布 (成功確率 p = 0.5)
bernoulli <- data.frame(x = 0:1, y = dbinom(0:1, size = 1, prob = 0.7))

# 二項分布 (n = 10, p = 0.5)
binomial <- data.frame(x = 0:10, y = dbinom(0:10, size = 10, prob = 0.5))

# ポアソン分布 (λ = 3)
poisson <- data.frame(x = 0:10, y = dpois(0:10, lambda = 3))

# ベルヌーイ分布、二項分布、ポアソン分布をそれぞれプロット
p1 <- ggplot(bernoulli, aes(factor(x), y)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Bernoulli Distribution") +
  theme_minimal()

p2 <- ggplot(binomial, aes(x, y)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Binomial Distribution") +
  theme_minimal()

p3 <- ggplot(poisson, aes(x, y)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Poisson Distribution") +
  theme_minimal()

# 3つのプロットを横に並べて表示
grid.arrange(p1, p2, p3, ncol = 3)
```


- 連続確率分布：確率密度関数
  - 連続した値をとる（e.g., 身長）
  - 正規分布、*t* 分布、カイ二乗分布

```{r, echo=FALSE}
# 正規分布 (平均 0, 標準偏差 1)
normal <- data.frame(x = seq(-4, 4, length.out = 100), 
                     y = dnorm(seq(-4, 4, length.out = 100), mean = 0, sd = 1))

# t分布 (自由度 10)
t_dist <- data.frame(x = seq(-4, 4, length.out = 100), 
                     y = dt(seq(-4, 4, length.out = 100), df = 10))

# カイ二乗分布 (自由度 5)
chisq_dist <- data.frame(x = seq(0, 15, length.out = 100), 
                         y = dchisq(seq(0, 15, length.out = 100), df = 5))

# 正規分布、t分布、カイ二乗分布をそれぞれプロット
p1 <- ggplot(normal, aes(x, y)) + 
  geom_line() + 
  ggtitle("Normal Distribution") +
  theme_minimal()

p2 <- ggplot(t_dist, aes(x, y)) + 
  geom_line() + 
  ggtitle("t Distribution") +
  theme_minimal()

p3 <- ggplot(chisq_dist, aes(x, y)) + 
  geom_line() + 
  ggtitle("Chi-Square Distribution") +
  theme_minimal()

# 3つのプロットを横に並べて表示
grid.arrange(p1, p2, p3, ncol = 3)
```


## 推測統計学の考え方

### 標本統計量
- 標本から計算される記述統計量のこと

- **標本平均**：母集団から得た標本のデータの平均値
  - **母平均**：母集団の平均値

### 標本分布

- 標本統計量が確率的に変動することを表した確率分布のこと

  - 標本を沢山抽出した場合であって、1つのデータの分布ではない
  
- 標本サイズ（ *k* ）が大きくなるにつれ、標本分布の散らばりはどんどん小さくなる

  - 以下の図では100ほどのデータがあれば母平均（0.7）の ± 0.1にほとんどのデータが集まることが分かる
  
  - しかし、これは母平均を事前に把握している場合を例であり、事前に母平均を知っていることは基本的にない。

```{r, echo=FALSE}
# セットアップ
set.seed(123)
n <- 10000 # 生成する二項分布に従う乱数の個数
theta <- 0.7

# ベルヌーイ試行の回数ごとに二項分布を生成し、ggplot2でヒストグラムを描く
k_values <- c(1, 10, 50, 100)
plots <- list()

for (k in k_values) {
  # 二項分布に従う乱数の生成
  data <- rbinom(n = n, size = k, prob = 0.7)
  
  # ggplotでヒストグラムを作成
  p <- ggplot(data.frame(x = data/k), aes(x)) + 
    geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
    ggtitle(paste("Binomial Distribution (k =", k, ")")) +
    theme_minimal() +
    scale_x_continuous(labels = scales::label_comma()) 
  
  plots[[length(plots) + 1]] <- p
}


plots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] 
```

## 区間推定の考え方
### 母集団分布に確率分布を仮定する
- 母集団分布が未知の場合、母集団からある値が標本として選ばれる事象の確率が。よく知られた確率分布に従うと仮定する。

  - 統計モデル（statistical model）や確率モデル（probabilistic model）と呼ぶ
  
  - よく知られたモデルとして、**正規分布（normal distribution）**が挙げられる

- 適切な確率モデルを設定しなければ、そこから算出したデータは正確ではない。

  - ある程度大きいデータを取り、そのヒストグラムを描画して形状を確認する
  
  - データの発生メカニズムから選ぶ
    
    - 例. 反応速度は対数正規分布で近似する、発生確率の低い出来事はポワソン分布に近似する

### 正規分布の性質
- 確率分布は形状を決めるパラメータがある。

- 正規分布のパラメータ：平均（確率分布の位置） + 分散（確率分布の広がり）

  - 期待値（Expected value）：確率分布の平均値のこと

```{r, echo=FALSE}
# パラメータの設定
mu_values <- c(0, 0, 0, -2)  # 平均 (0, 2, -2, 0)
sigma_values <- c(0.2, 1, 5, 2)  # 標準偏差 (1, 1, 1, 2)

# x軸の範囲を設定
x <- seq(-10, 10, length.out = 1000)

# データフレームを作成してggplotで描画
data <- data.frame(
  x = rep(x, 4), 
  y = c(dnorm(x, mean = mu_values[1], sd = sigma_values[1]), 
        dnorm(x, mean = mu_values[2], sd = sigma_values[2]), 
        dnorm(x, mean = mu_values[3], sd = sigma_values[3]), 
        dnorm(x, mean = mu_values[4], sd = sigma_values[4])),
  distribution = rep(c("mu=0, sigma=0.2", "mu=0, sigma=1", "mu=0, sigma=5", "mu=-2, sigma=2"), each = length(x))
)

# ggplotで正規分布曲線を描画
ggplot(data, aes(x = x, y = y, linetype = distribution)) + 
  geom_line(size = 1) +
  labs(title = "Different Normal Distributions", 
       x = "x", 
       y = "Density") +
  theme_minimal() 
```

- 確率変数が離散的な値をとる場合、確率変数の実現値ろと確率は一対一対応している（例 1か0のデータ）。

```{r, echo=FALSE}
# パラメータ設定
set.seed(123)
n <- 10000  # 乱数の個数
size <- 10   # 試行回数
prob <- 0.7  # 成功確率

# 2項分布に従う乱数の生成
data <- rbinom(n, size, prob)

# ggplotでヒストグラムを作成
ggplot(data.frame(x = data), aes(x = x)) + 
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black", aes(y = ..density..)) + 
  labs(title = "Binomial Distribution (Size = 10, p = 0.7)",
       x = "Number of Successes",
       y = "Density (Probability)") +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, size, by = 1)) 

```

- 確率変数が連続的な値をとる場合、実現値の値ではなく、その**範囲と確率**が対応する。つまり、確率分布の面積が確率と対応する（数値積分をして求めないといけない）。

```{r, echo=FALSE}
set.seed(123)
n <- 100000  # 乱数の個数

data <- data.frame(value = rnorm(n, mean = 0, sd = 1))

# 密度推定
density_data <- density(data$value)
density_df <- data.frame(x = density_data$x, y = density_data$y)

# ggplotで描画
ggplot(density_df, aes(x = x, y = y)) +
  geom_line(color = "black") +  # 密度曲線
  geom_ribbon(data = subset(density_df, x >= 0 & x <= 1), 
              aes(ymin = 0, ymax = y), 
              fill = "skyblue", alpha = 0.7) +  # xが0から1の範囲を塗りつぶし
  labs(title = "青で塗られているのは0から1の範囲",
       x = "Value",
       y = "Density") +
  theme_minimal()
```

```{r}
integrate(f = dnorm, lower = 0, upper = 1)
```

- 平均値 ± **1.96** × 標準偏差で、標本分布の95%の範囲になる。
- 白い部分は二か所合わせてデータの5%
```{r, echo=FALSE}
set.seed(123)
n <- 100000  # 乱数の個数

data <- data.frame(value = rnorm(n, mean = 0, sd = 1))

# 密度推定
density_data <- density(data$value)
density_df <- data.frame(x = density_data$x, y = density_data$y)

mean(density_data$x) + 1.96

# ggplotで描画
ggplot(density_df, aes(x = x, y = y)) +
  geom_line(color = "black") +  # 密度曲線
  geom_ribbon(data = subset(density_df, x >= mean(density_data$x) - 1.96 & x <= mean(density_data$x) + 1.96), 
              aes(ymin = 0, ymax = y), 
              fill = "skyblue", alpha = 0.7) +  # xが0から1の範囲を塗りつぶし
  labs(title = "青で塗られているのは分布の95%",
       x = "Value",
       y = "Density") +
  theme_minimal()
```
- mean(density_data$x) ± 1.96となっているのは、標準偏差が1であるため。
```{r}
integrate(f = dnorm, lower = mean(density_data$x) - 1.96, upper = mean(density_data$x) + 1.96)
```

### 確率モデルを用いた推定
- 適切な確率モデルが設定されているとき、母数を推定することは、確率モデルのパラメータがどのような値であるかを推測することと同じ。

### 正規分布モデルにおける標本平均の標本分布

- **標準誤差（Standard Error: *SE*）**

  - 標本分布の標準偏差

SE=\frac{\sigma}{\sqrt{n}}

- *n* = 標本サイズ 

  - 標本サイズが大きいほど標準誤差は小さくなり、標本平均は母平均に近づく（分母に標本サイズ）

### 区間推定
- **点推定（point estimation）**：平均値のように1つの値によって母数（母平均など）を推定すること

  - 標本サイズがどれだけ大きくても、標本平均は確率的に変動するため、ずばり母数を当てることは難しい

#### 信頼区間

- **区間推定（interval estimation）**：区間による推定を行うこと

  - 必ずこの区間に当てはまることは主張できない。そのため、区間推定では、**信頼度（degree of confidence）**を設定する。これを**信頼区間（confidence interval）**という。
  
    - 言語研究の分野では95%が一般的

##### 母分散を事前に知っている場合

- 標本サイズは25、標本平均99.46、母分散は4を例とする。よって標準誤差は0.4。

  - 「得られた標本統計量が95%の信頼度で否定されない母数の範囲」を考える => 標本平均99.46が95%上限ぎりぎりになるような母数と、95%下限ぎりぎりになるような母数の範囲。上記で扱ったように、面積の95%を塗りつぶすには、平均値から ± 1.96 × 標準偏差すればよい。
  
  - 下限：99.46 - 1.96 × 0.4 = 98.68
  - 上限：99.46 + 1.96 × 0.4 = 100.24

    - 下記の図の赤い点線の内側の範囲が95%信頼区間
```{r, echo=FALSE}
set.seed(123)

# パラメータ設定
mean1 <- 98.68  # 第一の平均
mean2 <- 100.24 # 第二の平均
sd_val <- 0.4     # 標準偏差（仮定）

# x 軸の範囲を設定
x_vals <- seq(mean1 - 1.96*sd_val, mean2 + 1.96*sd_val, length.out = 200)

# 正規分布の密度関数を計算
data <- data.frame(
  x = rep(x_vals, 2),
  y = c(dnorm(x_vals, mean = mean1, sd = sd_val), 
        dnorm(x_vals, mean = mean2, sd = sd_val)),
  distribution = rep(c("Mean = 98.68", "Mean = 100.24"), each = length(x_vals))
)

x <- data %>%
  dplyr::filter(distribution == "Mean = 98.68") %>%
ggplot(aes(x = x, y = y)) +
  geom_line(size = 1) +
  labs(title = "Mean = 98.68",
       x = "Value",
       y = "Density") +
  geom_vline(xintercept = mean1, linetype = "dashed", color = "red") +  # 下限
  geom_vline(xintercept = (mean1 + mean2)/2, linetype = "solid", color = "black") + # 標本平均
  #geom_ribbon(data = subset(data, x >= mean1 - 1.96*sd_val & x <= mean1 + 1.96*sd_val), 
  #            aes(ymin = 0, ymax = y), fill = "skyblue", alpha = 0.7) +
  theme_minimal()

y <- data %>%
  dplyr::filter(distribution == "Mean = 100.24") %>%
ggplot(aes(x = x, y = y)) +
  geom_line(size = 1) +
  labs(title = "Mean = 100.24",
       x = "Value",
       y = "Density") +
  geom_vline(xintercept = mean2, linetype = "dashed", color = "red") +  # 下限
  geom_vline(xintercept = (mean1 + mean2)/2, linetype = "solid", color = "black") + # 標本平均
  #geom_ribbon(data = subset(data, x >= mean2 - 1.96*sd_val & x <= mean2 + 1.96*sd_val), 
  #            aes(ymin = 0, ymax = y), fill = "skyblue", alpha = 0.7) +
  theme_minimal()

# 描画
z <- ggplot(data, aes(x = x, y = y, color = distribution, linetype = distribution)) +
  geom_line(size = 1) +
  labs(title = "Comparison of Two Normal Distributions",
       x = "Value",
       y = "Density") +
  scale_color_manual(values = c("black", "black")) +
  geom_vline(xintercept = mean1, linetype = "dashed", color = "red") +  # 下限
  geom_vline(xintercept = mean2, linetype = "dashed", color = "red") +  # 上限
  geom_vline(xintercept = (mean1 + mean2)/2, linetype = "solid", color = "black") + # 標本平均
  theme_minimal() +
  theme(legend.position = "none")  # ← 凡例を非表示  

x + y - z + plot_layout(ncol = 1)
```


##### 母分散を知らない場合

- 母分散が分からない場合の母平均の推定方法を扱う。現実的に、母分散が分かっていることはほとんどない。

- 母分散が分からない場合、上記の算出法では母平均の区間推定はできない。従って、母分散に依存しない統計量の計算が必要となる

###### *t* 値を用いた信頼区間の計算
- ***t* 値**：下記の式から算出できる値。*t*分布に従う。

\[
t = \frac{\bar{x} - \mu}{\frac{u^2}{\sqrt{n}}}
\]


  - *t*分布には、**自由度（degree of freedom）**と呼ばれるパラメータがある。*t*分布の自由度は、標本サイズによって変わる。自由度は、 *n* - 1で計算できる。
  - 自由度が100くらいになると、標準正規分布（平均0, 標準偏差1の正規分布）と同じような形になる。

- 母平均が分からない場合、上記の*t*値を計算することはできない。しかし、母平均の信頼区間を求める場合、*t*分布の95%の面積にああるような範囲を考えればいい。自由度25の時、上側と下側それぞれ2.5%（5% ÷ 2）となるような*t*値を求める（表から値を探したりして値を見つける）。

- Rの標準的な関数で求めることも可能
```{r}
qt(.975, df = 24)
qt(.025, df = 24)
```

以下の式をもとに算出する
  
\[
\left( \bar{x} - t_{\alpha/2, \, n-1} \cdot \frac{u^2}{\sqrt{n}}, \quad \bar{x} + t_{\alpha/2, \, n-1} \cdot \frac{u^2}{\sqrt{n}} \right)
\]


- *u^2*は不偏分散であり、以下の式で求められる。自由度25、標準偏差が2.24の場合、*n*に25、*S*に2.24を代入する。しかし、標本サイズが大きいときはほとんど同じ値になるため、実用上標本分散を用いることもある。

\[
s^2 = \frac{1}{n-1}S^2
\]

- 上限：99.46 + 2.06 * sqrt\left\{\frac{5.23}{25}\right\}
- 下限：99.46 - 2.06 * sqrt\left\{\frac{5.23}{25}\right\}

  - 98.52 - 100.40の間に母平均があると推定された

## Rの関数で95%信頼区間を求める

```{r}
gmodels::ci(mtcars$mpg)
```

- 上記の関数と一致した
```{r}
mean(mtcars$mpg) + qt(df = (nrow(mtcars)-1), .025) * sd(mtcars$mpg)/sqrt(nrow(mtcars))
mean(mtcars$mpg) + qt(df = (nrow(mtcars)-1), .975) * sd(mtcars$mpg)/sqrt(nrow(mtcars))
```

::: infobox

**不偏分散の不偏とは**
　推測統計学では**一致性（consistency）**、**不偏性（unbiasedness）**、**有効性（efficiency）**という概念が重要となります。詳しい説明は別の機会に行いますが、不偏分散は不偏性をもつ推定量のことを表します。不偏性とは推定量の期待値が母数と一致する性質になります。標本平均は不偏推定量ですが、標本分散、標本標準偏差は不偏推定量ではありません。つまり、母分散と標本分散にはズレが存在します。このズレを調節するため、n - 1で偏差二乗和を割っています（割る数が-1だけ減るので、標本分散より少し大きくなる）。
:::

## ハンズオンセッション

### データの読み込み
```{r}
dat <- read.csv("../stat_class_2025/sample_data/pokemon_data.csv")
```

#### データの中身を確認
```{r}
summary(dat)
```

```{r}
head(dat)
```

### 確率分布を選ぶ

#### データの可視化
```{r}
hist(dat$こうげき)
```

#### 正規分布をあてはめてみる
```{r}
# x軸の範囲を決定
x <- seq(min(dat$こうげき), max(dat$こうげき), length.out = nrow(dat)) 
#min(dat$こうげき) から max(dat$こうげき) まで、データの数 (nrow(dat)) だけ 等間隔 に区切った値を x に入れる。これを使って、なめらかな曲線を描画するための基準点を作る

# ヒストグラムを確率密度で描画
hist(dat$こうげき, probability = TRUE)
#probability = TRUE を指定すると、ヒストグラムの y軸が「確率密度」になる。分布をあてはめるときは、この設定をしないと当てはまらない。デフォルト（probability = FALSE）では「度数」（データの数）になる

# 正規分布の確率密度関数を描画
lines(x, dnorm(x, mean = mean(dat$こうげき), sd = sd(dat$こうげき)), lwd = 2)
#「データの分布が もし正規分布に従っていたら どんな形になるか？」を、ヒストグラムの上に描く。dnorm() は 正規分布 の確率密度関数を計算する。平均 (mean(dat$こうげき)) と 標準偏差 (sd(dat$こうげき)) を使って 「理想的な正規分布」 の形を作る。lwd = 2で線の太さを少し太くする
```

### 母分散未知の母平均の95%信頼区間を求める

- 母平均を得られたデータから推定しよう

  - ポケモンのデータを読み込み、ci関数とqt関数で95%信頼区間求めてみよう。
  
    - qt関数では、*t*値を使い、式を作って計算する必要があります

### ci関数
- 今回の警告メッセージは無視してOK（[参考](https://stackoverflow.com/questions/64507040/calculating-confidence-interval-using-ci-function)）
```{r}
#install.packages("gmodels")
library(gmodels)
ci(dat$こうげき)
```

### qt関数
- 必要なパーツ：平均値、*t*値、標本標準偏差（本当は\[{\sqrt{不偏分散}}\] = 不偏標準偏差）、サンプル数

  - sd関数は、標準偏差を算出する関数だが、分母をn-1した値を返している（= 不偏標準偏差）
```{r}
n <-  nrow(dat) #サンプル数
m <- mean(dat$こうげき) #平均値
t_value <- qt(df = n - 1, .975) #t値
sd <- sd(dat$こうげき)#標本標準偏差

lower <- m - t_value * sd/sqrt(n) #下限
upper <- m + t_value * sd/sqrt(n) #上限

lower
upper
```

- 下限はこっちでも同じ値になる
```{r}
t_value_low <- qt(df = n -1, .025)
lower2 <- m + t_value_low * sd/sqrt(n)

lower2
```

## まとめ

<!--chapter:end:03-week3.Rmd-->

# 統計的検定

```{r, include=FALSE}
library(tidyverse)
library(patchwork)
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 統計的検定の論理や手順を理解できる
2. *t* 検定をRで実装し、分析結果を報告することができる

## 統計的検定（statistical test）
- 母数についての推定（例「日本人の男子の平均身長は170 cmだ」）が合っているかを、統計学的に検定する

  - 使用する標本は、母集団から無作為抽出していることを前提にしている = 標本は母集団を代表している

- 大別して2種類ある

  - **パラメトリック検定**： 母数（パラメータ）に特定の確率分布の仮定を設ける検定（例 *t* 検定、分散分析）
    - 注：確率分布の形状を決める値（正規分布の平均と標準偏差など）もパラメータと呼ぶ。本講義ではパラメータは母数ではなく、確率分布を規定する値を指す。
  
  - **ノンパラメトリック検定**：母集団に特定の確率分布を仮定しない検定（例 カイ二乗検定）
  
### 統計的検定の論理
- 母集団からサンプリングした標本から、母数についての推定が合っているかを議論できない。しかし、確率的な範囲であれば、推定が間違っているかを確かめることができる。

  - 標本から計算した95%信頼区間が160 cmから167cmだった場合、母数は170cmよりも小さい（推定は誤り）
  - 標本から計算した95%信頼区間が167 cmから175cmだった場合、推定が合っているか誤りなのか分からない（信頼区間が170 cmを含んでいるから）

- 推測統計学における統計的検定の論理では、「母数についての等号（ = ）の仮説」が偽であることは言えるが、正しいとは言えない。

- 「母数の等号についての仮説」を**帰無仮説（null hypothesis）**と呼ぶ
  
  - 帰無仮説は以下のように表す（*H*はhypothesisの頭文字）\[H_0: u = 170\]

  - 統計的検定では、帰無仮説が偽であるかを検定する。
  
    - **背理法**に似た方法であり、証明したい主張の否定の仮説を立て、得られた結果から主張できる結論が「矛盾」であることを示すことで、主張が正しいことを証明する。
    
      - 例：「世界には白いカラスがいる」と主張したい
      - 「全てのカラスは黒い」という仮説を立てる（**帰無仮説**）
      - 1羽でも白いカラスが見つかれば、帰無仮説は偽であり、「世界には白いカラスがいる」と主張できる。
        - 仮説が偽と主張する（否定する）方が簡単
        
- **対立仮説**：帰無仮説の否定

\[H_1: u ≠ 170\]

  - 帰無仮説が否定されれば、対立仮説が真となる
  
  - 帰無仮説が偽であると判断できない場合、帰無仮説は保留になる（= 分からない）。
  
    - 帰無仮説は真であるということはできない。
    
### 統計検定の具体的な考え方

- 帰無仮説が正しいと仮定する。標本から計算された統計量の実現値が、標本分布から考えて**十分低い**確率でしか生じないような値であれば、帰無仮説が偽であると判断する

  - 注意点として、選んだ確率モデル（確率分布）が妥当でない場合、検定の結果も妥当ではなくなる。以下の例では、母集団からの標本抽出が正規分布で近似できるという仮定のもと行う。
  
- 例：
  2025年度にT先生のクラス（40人）で実施された英語テストの平均点は60点であった。昨年度の平均点は55点であり、2025年度の平均点は、昨年度の平均点に比べ十分に高い得点であるかを検定する。
  
  - 帰無仮説 *u* = 55の下で、得点が平均パラメータ *u* = 50の正規分布に従うと仮定。
  - クラス平均は60点、標準偏差は3であると分かった。
  - 母分散が分からない場合、*t* 統計量を用いる。

\[t = \frac{\overline{x} - \mu_0}{\frac{u}{\sqrt{n}}}\]

  - 上記の値を代入し、
  
\[t = \frac{\overline{60} - 55}{\frac{3}{\sqrt{40}}}\]

- 計算すると、 *t* = 10.54となる。このような検定のための統計量を**検定統計量**という。
```{r}
(60-55)/(3/sqrt(40))
```

- 自由度が39（40-1）で95%の範囲は0±2.02であるため、得られた*t*値がこの絶対値より大きいと棄却域に（青く塗られた箇所）に入ることになる。帰無仮説が真であると仮定したときの検定統計量の標本分布のことを**帰無分布（null distribution）**という。
- 棄却域が占める割合を**有意水準（significance level）**と言い、*α*と表記する。慣例的に、両側合わせて0.05が用いられる。

```{r}
qt(0.975, df = (40-1))
```

```{r, echo=FALSE}
# 自由度
df <- 39

# t分布の臨界値（両側検定, α = 0.05）
t_crit <- qt(0.975, df)  # 片側2.5%の閾値

# t分布のデータを作成
x_vals <- seq(-4, 4, length = 1000)  # x軸の範囲
y_vals <- dt(x_vals, df)  # 確率密度

# データフレームを作成
df_t <- data.frame(x = x_vals, y = y_vals)

# ggplotで描画
ggplot(df_t, aes(x, y)) +
  geom_line(color = "black") +  # t分布の曲線
  geom_area(data = subset(df_t, x >= t_crit), aes(x, y), fill = "blue", alpha = 0.5) +  # 右側棄却域
  geom_area(data = subset(df_t, x <= -t_crit), aes(x, y), fill = "blue", alpha = 0.5) + # 左側棄却域
  geom_vline(xintercept = c(-t_crit, t_crit), linetype = "dashed") +  # 臨界値の線
  labs(title = "自由度39のt分布と棄却域 (α = 0.05)", x = "t値", y = "密度") +
  theme_minimal()
```

- 得られた*t*値は2.02よりも大きく、棄却域に入る。従って、帰無仮説が真であると考えると、非常に小さい確率でしか起きない結果が得られたという帰無仮説が真であるという仮定と矛盾した結果となっている。この場合、**統計的に有意**であると表現する。

  - つまり、「2025年度の平均点60点は、昨年度よりも統計的に高い得点である。」と主張できる。
  
  
::: infobox
2025年度の得点が昨年度よりも統計的有意に高いことは、**全員の得点が昨年度よりも高いことを必ずしも意味しない**ということに注意。比較しているのはあくまでも**平均点**。以下のどちらのデータでも統計的有意差が得られる。

```{r, echo=FALSE}
set.seed(123) # 再現性の確保

# 条件1: すべての値が55点以上
data1 <- pmax(rnorm(40, mean = 60, sd = 3), 55)

# 条件2: 半分は55点未満
data2 <- rnorm(40, mean = 60, sd = 3)
data2 <- sort(data2) # 小さい順に並べる
data2[1:20] <- qnorm(runif(20, 0, pnorm(55, mean = 60, sd = 3)), mean = 60, sd = 3) # 下半分を55未満に調整

df <- data.frame(
  ID = 1:40,
  Data1 = data1,
  Data2 = data2
)

```

```{r}
summary(data1)
summary(data2)
```

- 黒い点線は平均値55点
```{r, echo=FALSE, message=FALSE}
a <- df %>%
  ggplot(aes(Data1)) +
  geom_histogram(fill = "skyblue", color = "black", alpha = 0.7, bins = 10) +
  geom_vline(xintercept = 55, lwd = 2, linetype = "dashed") +
  theme_minimal() +ggtitle("平均: 60.27")

b <- df %>%
  ggplot(aes(Data2)) +
  geom_histogram(fill = "salmon", color = "black", alpha = 0.7, bins = 10) +
  geom_vline(xintercept = 55, lwd = 2, linetype = "dashed") +
  theme_minimal() +ggtitle("平均: 58.11")

# 並べて表示
a + b
```
:::  

## 統計的検定の手順

1. 確率モデルの設定
- 母数について仮説を立てる。この母数は確率モデル（確率分布）のパラメータと対応しているという仮定が必要。

  - 先行研究の結果、得られたデータの分布などから適切な確率モデルを選ぶ必要がある
  
2. 帰無仮説の設定
- 例えば、映画を見ると英単語の学習に効果的だということを調べる場合、映画を見る群と、映画を見ない群を用意する。二つの群の平均値の差が0でなければ、映画を見ることに何らかな効果があると主張できる。この場合、以下のような帰無仮説を立てる。

  - 帰無仮説：二つの群の平均値差は0

3. 検定統計量の設定

4. 有意水準（ *α* ）の設定

- 慣例的に言語研究の分野では5%が用いられる。

- 帰無仮説を偽とする場合でも、最大5%程度はそれが間違えている可能性を表す。

  - 分析の前に決定し、結果を見て有意水準を大きくするのは、研究倫理に反するとみられる可能性が大いにある。
    - 分析の前に0.05よりも大きくすることはよい。

5. 検定統計量の実現値の計算

## 統計的検定で必要な知識

### *p* 値
- 検定統計量を変換し、有意水準と比較しやすくしたもの

  - 検定統計量の実現値が大きいほど*p*値は小さくなる
  
- 上記の手順では、検定統計量の実現値と棄却域の範囲を確認して、帰無仮説が棄却されるかを判断していた。しかし、コンピュータソフトウェアでは、*p*値が表示され、この値が事前に設定した有意水準よりも大きいかもしくは小さいかを確認して棄却の有無を判断する。

```{r}
t.test(df$Data1, mu = 55)
t.test(df$Data2, mu = 55)
```

::: infobox
e-やe+は指数的記法です。6.106e-05であれば、6.106 × 10^(-5)なので、0.を合わせて0が5つ6の前につきます。逆に6.106e+05であれば6.106 × 10^5です。Rでは```options```関数で通常の値に戻して表示できます。

```{r}
options(scipen = 999)
t.test(df$Data2, mu = 55)
```
:::

### 統計的検定の誤りと検出力

  
- 第一種の誤り（type Ⅰ error）

  - 帰無仮説が真なのに、誤って偽だと主張すること（本当は差がないのに差があると判断する）
  
  - 有意水準 *α* の値と一致
  
    - 統計的検定では、限界点として、100%正しいことは保証されない。有意水準が5%であれば、5%の誤った主張を許すことになる。
  
- 第二種の誤り（type Ⅱ error）

  - 帰無仮説が偽なのに、それを偽であると言えない（保留）すること（本当は差があるのに差がないと判断する）

  - *β* で表す

```{r, echo = F, fig.cap = "image source: https://www.statisticssolutions.com/to-err-is-human-what-are-type-i-and-ii-errors/"}
frink <- magick::image_read("C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png")
magick::image_write(frink, "C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png", format = "png")
knitr::include_graphics("C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png")
```

- 検出力（power）

  - 帰無仮説が偽であるとき、正しく帰無仮説を棄却できる確率

  - 1 - *β* で表す
  
  - **統計的検定では、*α* を維持しながら検出力を高くするのが望ましい**
  
  - 大きい効果ほど、大きい標本サイズほど、統計的有意を検出しやすくなる
  
    - 有意水準を小さくすると、第二種の誤りが大きくなる = *α* を小さくすると、*β* は大きくなる
  
    - 帰無分布と真の母数の標本分布が離れるため、検出力は、帰無仮説と母平均の差が大きいほど、標本サイズが大きいほど大きくなる。

::: infobox
サンプルサイズ・検定力・有意水準に加え、**効果量（effect size）**も連動している。効果量は後の講義で詳しく扱うが、簡単に説明すると、関心を持つ事柄の大きさである。4つのうち、3つが分かれば、自動的に残りの1つの値を特定できる。これを利用したのが**検定力分析（power analysis）**である。これを**実験前に**使うことで、どれくらいの標本を収集すべきかを検討できる。近年の研究では、参加者の数を検定力分析で決めているかを重視する（査読で！）動きがある。

:::



### 有意性検定（≒ 統計的検定）の問題点

- 有意確率（ *p* < 0.05）だけに結果を頼るのはよくない

  - 無作為抽出を前提としていても、必ず得られた結果に**誤差**が含まれている
  
  - 標本サイズに大きく左右される

- 統計的検定における誤差と問題点の対処法

1. できるだけ母集団を代表するサンプルを得る
2. 信頼区間を報告する
3. 検定力分析で標本サイズを事前に決める
4. 効果量を報告する

::: infobox
言語研究における効果量は、Neyman-Pearson流の理論に基づくことがほとんど。
:::

### 頻度論的統計学とベイズ統計学
- ここまで説明してきた話 + この講義のほとんどの説明における統計学は、頻度論的統計学（frequentist statistics）と呼ばれる枠組み。

  - 検定において用いる数学的手法が異なる。頻度 vs. ベイズのような「主義の違いによる対立構造」は避けた方がよい
  
    - [参考: Bayesians are frequentists.](https://statmodeling.stat.columbia.edu/2024/01/08/bayesians-are-frequentists-2/)

  - 母集団からサンプリングする（標本を抽出する）のは繰り返すことができる試行において起こる事象の相対頻度（frequency）をもとに行われる。例えば、サイコロを14回降って、1の目が3回出た場合、1が出る確率は \frac{3}{4} になる。これを**頻度論的確率**と呼び、人間の主観や信念に依存しない客観的な確率である。
    
    - 確率とは無限回試行を行ったときの割合

    - 仮説の評価を *p* 値を用いて行う
    
      - 「帰無仮説のもとでデータが得られる確率」を計算する
      
    - データを追加して再度分析する際、、適切な方法を用いないと、計算された*p* 値と有意水準が当初の値から変わってしまう（検定力分析が重要な理由）

- **ベイズ統計学**では、**主観確率**（「明日は30%で晴れるだろう」）と客観確率（「10日3日晴れたから明日は晴れるだろう」）の両方を検定に使用する。

  - ベイズ統計学では *p* 値は使わない
  
  - ベイズ統計では、データを追加して再度分析することはOK（データの二度漬けをしない限り）

- 頻度論的統計学、ベイズ統計学どちらにおいても、「無作為抽出により標本が母集団を代表している」「母集団からのサンプリングを近似していること」という前提が重要

  - ベイズ統計学の方が道具的に使いやすいと考えることもできる

::: infobox

フローチャート的な統計分析は推奨されません。分析の前に、分析するデータの母集団がどのような形状なのか、この分析方が正しいのかなど十分検討することが重要です。

:::

## ハンズオンセッション
### *t* 検定
- *t* 分布に照らし合わせて、2群の平均値の差を検証する場合に用いる。

  - 平均値だけでなく、分散（データのばらつき）も考慮される

### 基本用語

#### 対応のあり・なし

- 対応あり（repeated measures）：同じ参加者からの2種類のデータ（例 同じ参加者の国語と英語の点数）

- 対応なし（independent measures）：異なる参加者からなる2種類のデータ（例 1年生と2年生の英語の点数）

  - 実験群（experimental group）と統制群（control group）

### *t*検定を使用する前提

1. データの種類：間隔尺度または比例尺度
2. サンプリング：母集団から無作為抽出され、母集団を十分代表していること
3. 正規性：標本平均の分布が正規分布に従うこと。少々外れている場合は正規性に対して頑健であるためOK。大きく外れている場合はノンパラメトリック版の*t*検定を検討することも考えれるが、ウェルチの*t*検定を用いることをできるだけ優先する方がいい場合が多い。ノンパラメトリック検定にも使用する前提がある。
4. （対応なしの場合のみ）等分散性：比較する2群のデータの分散が等しいこと（= 母分散が等しい集団からデータがサンプリングされていること）。*t*検定は母分散の等質性に関しても頑健であるため、グループのサンプルサイズが等しい場合、分析結果が歪むことはほとんどない。
  - ウェルチの*t*検定を使うのがよい = *t* 検定の場合基本ウェルチの*t* 検定を使う
  
5. （対応なしの場合のみ）観測値の独立性：データがお互いに影響し合い相関が高い場合、第一種の過誤が起こりやすくなる


<div class="alert alert-info">
  <strong>重要</strong>正規性の検定や等分散性の検定を行うことを推奨されることがあるが、このような事前テストは第一種&第二種の過誤の確率を高めることが報告されている（e.g., Rasch et al. [2011]）。そのため、そのような検定を行わず、ウェルチの*t* 検定を行う方がよい。
</div>



### *t* 値の算出

- 標本平均の標本誤差：差がどれだけ偶然の誤差によって起きるかを推定

- 偶然起こる誤差おりどの程度大きいかを調べる検定

\[t\ =\ \frac{観測された標本平均の差}{標本平均の差の標準誤差}\]

#### 対応なし *t* 検定（サンプルサイズが等しい）
\[
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}} \hspace{3pc} (df = n_1 + n_2 - 2)
\]


#### 対応なし *t* 検定（サンプルサイズが異なる）

\[
s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
\]

\[
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_p^2}{n_1} + \frac{s_p^2}{n_2}}}
\]


#### 対応あり *t* 検定

\[
t = \frac{\bar{x}_1 - \bar{x}_2}{S_D / \sqrt{n}} \hspace{3pc} (df = n - 1)
\]


### Rでの実装

#### 対応なし *t* 検定

- データの読み込み
```{r}
dat_t_ind <- read.csv("../stat_class_2025/sample_data/ttest_inde.csv")
```

```{r}
head(dat_t_ind)
```
```{r}
length(table(dat_t_ind$ID))
```


```{r}
boxplot(English ~ Class, data = dat_t_ind)
```

```{r}
library(psych)
describeBy(dat_t_ind$English, group = dat_t_ind$Class)
```



```{r}
t.test(English ~ Class, data = dat_t_ind)
```

- **論文記載例**

- \* 本来であれば、効果量も報告する必要があるが、今回は省略している。

  - 異なる指導法を実施したクラス A とクラス B の英語テストの平均点は、83.03 (*SD* = 4.3)と 60.48 (*SD* = 5.73 でクラス A の平均の方が高かった。 *t* 検定を使って比較した結果、*t*(72.353) = 19.911, *p* < .001、 *d* = xxx [95%CI = xxx, xxx] で、クラスAのほうが統計的に有意に英語テストの成績が高いことがわかった。

    - *p* 値は原則実数値報告である（ *p* = .046）。しかし、値が0.01よりも小さい場合は、*p* < .001のように報告する。

#### 対応あり

- データの読み込み
```{r}
dat_t_rep <- read.csv("../stat_class_2025/sample_data/ttest_rep.csv")
```

```{r}
head(dat_t_rep)
```

```{r}
length(table(dat_t_rep$ID))
```

```{r}
boxplot(dat_t_rep$English, dat_t_rep$Math, names = c("English", "Math"))
```

```{r}
describe(dat_t_rep$English)
describe(dat_t_rep$Math)
```
- ```paired = T```を設定する
```{r}
t.test(dat_t_rep$English, dat_t_rep$Math,
      paired = T
       )
```

- **論文記載例**

- \* 本来であれば、効果量も報告する必要があるが、今回は省略している。

  - Aクラスの40名の学生を対象に、英語と数学のテストの得点を比較した。英語の平均点は、82.22 (*SD* = 8.65)と 75.85 (*SD* = 10.02)で英語の平均の方が高かった。対応ありの *t* 検定を使って比較した結果、*t*(39) = 21.641, *p* < .001、 *d* = xxx [95%CI = xxx, xxx] で、英語のほうが統計的に有意に得点が高いことがわかった。

## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

- 心理学統計法　放送大学
- https://www.isc.meiji.ac.jp/~hirukawa/randomevent/test1.htm
- 中村 心理学・教育学研究のための効果量入門
- 平井 et al. 
- 心理学統計の基礎
- https://www.note.kanekoshobo.co.jp/n/nf836d37b7f10#61a8e679-85a4-411c-96dd-5a1a37335572
- https://x.com/genkuroki/status/1227224899875295234
- https://norimune.net/3339
- https://statmodeling.stat.columbia.edu/2024/01/08/bayesians-are-frequentists-2/
- Rasch, D., Kubinger, K. D., & Moder, K. (2011). The two-sample t test: pre-testing its assumptions does not pay off. Statistical papers, 52, 219-231.

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:04-week4.Rmd-->

# Week5: 相関分析

```{r, include=FALSE}
library(tidyverse)
library(patchwork)
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 相関分析に関する統計理論を数学的・概念的に理解できる。
2. 相関分析をRで行い、その結果を解釈し報告することができる。

## 相関分析（Correlation analysis）とは

- **相関**：2つのデータの間にある**線形**の関係の強さを示す。これを分析することを**相関分析**という

  - 相関関係を確認する際、散布図で視覚的に確認、数値的に**相関係数**（Correlation coefficienct）が用いられる。
  
    - 通常、**ピアソンの（積率）相関係数（Pearson's correlation coefficient, *r*）**を指す。値は \(-1\leq r \leq1\) をとる。
  
  - 相関係数は2変数の関係が線形になっている場合のみ使用できる。

- 相関係数の重要なパーツ

  - **共分散（covariance）**：各変数の平均からの偏差の積を平均したもの

\[
s_{xy} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
\]

- **不偏共分散**は以下の通り。相関係数を算出するRパッケージの```cor```は計算する際に、不偏分散を使用している。


\[
s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
\]

```
The denominator *n* − 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. 
```

- 相関係数を求める式

\[
r_{xy} = \frac{S_{xy}}{S_xS_y}
\]

- 共分散（\(S_{xy}\)）を2変数それぞれの標準偏差の積で割ったもの（共分散を基準化している）
  
  - 基準化の理由として、定数倍などすると、値の大きさが変わってしまうため
  
### 相関関係の種類

  - **正の相関**（positive correlation）：右上がり。1つの変数が増加すると、もう片方も増加する
  
    - 相関係数が正の値をとる
  
  - **負の相関**（negative correlation）：右下がり。1つの変数が増加すると、yが減少する。
  
    - 相関係数が負の値をとる
    

```{r, echo=FALSE}
set.seed(123)

# 相関係数のリスト（-1から1まで0.1刻み）
cor_values <- seq(-1, 1, by = 0.1)

# データを作成
generate_data <- function(r, n = 100) {
  x <- rnorm(n)
  y <- r * x + sqrt(1 - r^2) * rnorm(n)  # 指定した相関係数に従うyを作成
  data.frame(x = x, y = y, correlation = paste("r =", round(r, 1)))
}

# すべての相関係数に対してデータを作成
data_list <- lapply(cor_values, generate_data)
data <- bind_rows(data_list)

data.2 <- data %>%
  mutate(correlation = factor(correlation,
                              levels = c("r = -1", "r = -0.9", "r = -0.8", "r = -0.7", "r = -0.6",
                                     "r = -0.5", "r = -0.4", "r = -0.3", "r = -0.2", "r = -0.1",
                                     "r = 0", "r = 0.1", "r = 0.2", "r = 0.3", "r = 0.4", "r = 0.5",
                                     "r = 0.6", "r = 0.7", "r = 0.8", "r = 0.9", "r = 1"
                                     )))

# ggplotで可視化
ggplot(data.2, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  facet_wrap(~correlation, ncol = 5) +  # 5列で表示
  theme_minimal() +
  labs(title = "相関係数ごとの散布図",
       x = "X",
       y = "Y")
```

- ピアソンの相関係数は**外れ値**に影響されやすい。あらかじめ散布図で外れ値がないかを確認する
- 以下の図では、左側の図のデータの一部（*n* = 2）を外れ値に変更した
```{r, echo=FALSE, message=FALSE}
set.seed(123)

n <- 100
x <- rnorm(n, mean = 0, sd = 1)

data2 <- data.frame(
  x = rep(x, 2),
  y = 0.6 * x + rnorm(n, mean = 0, sd = 0.5)
  )

data4 <- data2 %>%
  mutate(y = case_when(y > 1.6 ~ y + 10, TRUE ~ y)) %>%
  mutate(x = case_when(y > 10 ~ x + 10, TRUE ~ x))

a <- ggplot(data2, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, color = "skyblue") +  # 散布図（透明度を少し下げる）
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # 回帰直線
  #coord_fixed(ratio = 1) +  # **正方形のプロット**
  theme_minimal(base_size = 15) +  # シンプルなテーマ
  labs(x = "X", y = "Y")

b <- ggplot(data4, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, color = "skyblue") +  # 散布図（透明度を少し下げる）
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # 回帰直線
  #coord_fixed(ratio = 1) +  # **正方形のプロット**
  theme_minimal(base_size = 15) +  # シンプルなテーマ
  labs(x = "X", y = "Y")

a + b
```

- 右側の図の相関係数がわずか2つの外れ値によって大きくなっていることが分かる
```{r}
cor(data2$x, data2$y, method = "pearson")

cor(data4$x, data4$y, method = "pearson")
```

### 相関係数の検定
- ほとんどの場合、母相関係数 *ρ* が0という帰無仮説を立てて行われる。

\[
H_0 : ρ = 0
\]

- 検定統計量は *t* 分布に従うことを利用して検定が行われる。

\[
t = \frac{r}{\sqrt{1-r^2}}\sqrt{n-2}
\]

- 統計的に有意である場合、「母相関係数は0である」という帰無仮説を棄却し、「母相関係数は0ではない」という対立仮説を採択する。**無相関検定**とも呼ぶ。

  - 母相関が0以外である帰無仮説を立てることもできるが、この場合検定統計量 *t* は **非心 *t* 分布**に従い、 *t* 分布には従わない。
  
<div class="alert alert-info">
  <strong>重要</strong> 無相関検定で有意差が得られても、母相関係数が0ではないということを示すだけであり、相関の強弱を示していないことに注意が必要。
</div>

## ハンズオンセッション

### データの読み込み

```{r}
dat <- read.csv("../stat_class_2025/sample_data/cor.csv")
```

```{r}
head(dat)
```
```{r}
library(psych)
describe(dat$English)
describe(dat$Math)
describe(dat$Japanese)
```


### 散布図の作成

```{r}
plot(dat$English, dat$Math)
plot(dat$English, dat$Japanese)
```

```{r}
psych::pairs.panels(dat[,2:4])
```

### 相関係数の算出

- ```corr.test```関数は、上述の```cor```関数を使用している。

```{r}
psych::corr.test(dat[,2:4], method = "pearson")
```

- ```cor.test```を使用するとより詳細な結果が返ってくる

```{r}
library(stats)
cor.test(dat$English, dat$Math, method = "pearson")
```


### 相関係数の解釈

#### 相関係数の大きさ
- Cohenの基準と呼ばれるベンチマークがある。しかし、これはもともとすべての分野に適応することを意図したものではなく、行動科学分野の研究に基づき作成されたものであった。

  - 小（small）： .10、中（Medium）：.30、大（Large）：.50

- 相関係数の値の大きさは文脈（e.g., 研究分野や研究対象）において解釈する必要がある。

  - 例）リーディングのテスト得点とリスニングのテスト得点が *r* = .50の場合と、聞いた音を書きだすテストの得点と聴解のテスト得点が *r* = .50

- 第二言語習得研究における相関係数の目安として提示された例
  
>"For correlation coefficients, we suggest that rs close to .25 be considered small, .40 medium, and .60 large. These values correspond roughly to the 25th, 50th, and 75th percentiles in our primary and meta-analytic samples." (Plonsky & Oswald, 2014)

- 相関係数が小さい研究は査読の段階で削除されていたり、そもそも出版されずお蔵入りになっている場合もあるので注意。

#### 因果関係の主張
- 相関分析では関係の強さの程度しか分からない。相関があることは必ずしも因果関係があることを示しているわけではない。

  - e.g., [チョコレートの消費量とノーベル賞受賞者の数に相関がみられた](https://www.biostat.jhsph.edu/courses/bio621/misc/Chocolate%20consumption%20cognitive%20function%20and%20nobel%20laurates%20(NEJM).pdf)
  
    - チョコレートを沢山食べると優秀な人材が育つんだね（✖ チョコレート→ ノーベル賞）

- 第三の変数（共変量）による**疑似相関**の可能性もある

  - チョコの例も、「豊かさ」による疑似相関かもしれない
  
    - 疑似相関の例：アイスクリームとビールの売り上げにおける「気温」、年収の高さと血圧における「年齢」

  - 第三の変数を考慮した相関係数の計算は回帰分析の回で扱います
  
- 因果関係を調べるための条件
  - ジョン・スチュアート・ミルは、以下の3つの条件を提示した
  
  1. 原因 *X* が結果 *Y* よりも時間的に先行している
  
  2. 原因 *X* と結果 *Y* に共変関係がある
  
  3. 他の因果的説明が排除されている

### 論文への記載

- 検証する変数が多い場合、表として提示すると分かりやすい。
- 特に焦点を当てたい個所を本文で言及する。

```{r, echo=FALSE, warning=FALSE, eval=FALSE}
#library("apaTables")
apa.cor.table(data = dat[,2:4], landscape = F,
               table.number = 1, show.conf.interval = TRUE, show.sig.stars = T)

sjPlot::tab_corr(data = dat, corr.method = "pearson")
```

**表 1 **
 
英語、数学、日本語のテスト得点の記述統計と相関係数、95%信頼区間

| Variable   | *M*     | *SD*    | 1      | 2        |
|------------|------|------|--------|----------|
| 1. English | 49.45 | 13.19 |        |          |
| 2. Math    | 49.50 | 14.09 | .64** [.41, .80] |          |
| 3. Japanese | 49.35 | 13.41 | .69** [.48, .82] | .43** [.14, .65] |

**注.**  * *p* < .05. ** *p* < .01.

- 3つのテスト得点の間には、統計的有意な相関関係がみられた（表1）。英語と日本語の相関係数が最も高く（*r* = .69 95% CI [.48-.82], *t*(38) = 5.90, *p* < .001）、数学と日本語の相関係数が最も低かった（*r* = .43,  95% CI [.14-.65], *t*(38) = 2.93, *p* = .006）。

## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

- 心理学統計法　放送大学
- 心理学・教育学研究のための効果量入門
- 平井 et al. 

- 外国語教育ハンドブック
- 心理統計
- Plonsky, L., & Oswald, F. L. (2014). How big is “big”? Interpreting effect sizes in L2 research. Language learning, 64(4), 878-912.
- Messerli, F. H. (2012). Chocolate consumption, cognitive function, and Nobel laureates. N Engl J Med, 367(16), 1562-1564.
- https://www.pref.yamaguchi.lg.jp/soshiki/22/101008.html

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:05-week5.Rmd-->

# 単回帰分析（連続型の説明変数）

```{r, include=FALSE}
library(tidyverse)
library(magick)
library(rstanarm)
options(scipen = 999)
```

## 今後の修正案

- 最小二乗法の話だけをするのであれば、lmを一貫して使った方がいい？でも正規分布なら一致するからglmを使ってもいい？？

- 興味深い資料：
https://qiita.com/s-nakagawa2/items/c01650b49fbda218a73d#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE
https://note.com/kiyo_ai_note/n/n8112cc3a665b

## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 回帰分析の統計理論を数学的・概念的に理解できる
2. 回帰分析（独立変数が連続値）をRで実装し、その結果を可視化などを通して説明できる

## 回帰分析（regression analysis）とは

- データに回帰直線をあてはめ、そこから得られた予測値や残差をもとにデータを解釈する方法

  - 青い線が回帰直線
  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
dat <- read.csv("../stat_class_2025/sample_data/pokemon_data.csv")

dat %>%
  ggplot(aes(x = こうげき, y = HP)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  coord_fixed(1 * max(dat$こうげき) / max(dat$HP))

```

### 回帰分析の種類
- 線形モデル（Linear Model）

  - 正規分布を仮定
  - 最小二乗法で推定
  
- 一般化線形モデル（generalized linear model）

  - 正規分布以外を仮定できる
  - 最尤推定法で推定
  
- 一般化線形混合（効果）モデル（generalized linear mixed model）
  - 正規分布以外を仮定でき、個体差や場所差などを考慮に入れる
  - 最尤推定法で推定

### 変数の名前

- 独立変数（independent variable）：説明変数や予測変数とも呼ばれる。予測に用いられる変数 *x*

- 従属変数（dependent variable）：目的変数や基準変数とも呼ばれる。予測される方の変数 *y*

### 最小二乗法

以下の一次式で表される


\[
\hat{y} = a + bx
\]

  - 傾き（ *b* ）：変数 *x* の<u>1単位の差異</u>に対応する *y* の予測値の差異の大きさ。**回帰係数（regression coefficient）** とも呼ばれる。
  
    - 変数 *x* の単位を把握しておくことが重要。標準化すると、1標準偏差あたりに変換できる
  
  - 切片（ *a* ）

- 回帰直線が実際のデータに最もよく適合するように計算される。計算方法として**最小2乗法（least squares method）**が1例としてあげられる。

\[
b = r\frac{S_y}{S_x}
\]

\[
a = \hat{y} - b\bar{x}
\]

- 回帰直線の式に上記の式を代入。*x* が平均 \(\bar{x}\) に等しいとき、 *y* の予測値 \(\hat{y}\) は *y* の平均 \(\bar{y}\) に等しくなる。= 回帰直線は、点（\(\bar{x}\), \(\bar{y}\)）を通る傾き *b* の直線

\[
\hat{y} = (\bar{y} - b\bar{x}) + bx\\
\hspace{0em} = \bar{y} + b(x - \bar{x})
\]


::: infobox
最小2乗法以外にも最尤推定法があり、前者は手元のデータに当てはめることを考え、後者はこのデータが出てくる確率が一番高くなるように、確率分布のパラメータを考える。どちらの基準を用いても推定値はほぼ同じ値になる。誤差が独立していて、正規分布していれば、最小二乗法と最尤推定は等価になる。最尤推定法は正規分布以外の確率分布にも適用できるため、最尤推定法の方がより用いられる（最小2乗法は2つの変数の直接的な関係を仮定するため、線形ではない回帰モデルでは最小2乗法が使えない場合がある）。

- 最小二乗法：```lm```関数
```{r}
stats::lm(mtcars$mpg ~ mtcars$cyl, weights = NULL)
```

- 最尤推定法：```glm```関数

  - iteratively reweighted least squaresを用いて最尤推定している
  
```{r}
stats::glm(mtcars$mpg ~ mtcars$cyl)
```

### ベイズ推定
- 最尤法の代わりに使う方法。

  - 最小2乗法、最尤推定法：「点」を推定 => 1つの値を返す

  - ベイズ推定：「幅」（分布）を推定。事後分布を返す。この分布の一番高い個所を「点」として報告することもできる

- マルコフ連鎖モンテカルロ法（MCMC）によって得られた乱数のサンプルを使って推定する。

```{r, echo=FALSE}
res <- rstanarm::stan_glm(
        mpg ~ cyl, 
         seed = 123,
         data = mtcars,
         refresh = 0
         ) #priors weakly informative by default

summary(res)
```
```{r, echo=FALSE}
posterior_samples <- as.matrix(res, pars = "cyl")# "am" の事後分布サンプルを取得
hist(posterior_samples)
```

- ベイズ推定では、事前分布を指定する。つまり、最小2乗法や最尤推定法を使う場合と異なり、得られたデータだけでなく、事前の知識を反映して分析できる。

  - 変数 *x*により狭い事前分布を設定平均が0、標準偏差が0.2

```{r, echo=FALSE}
set.seed(123)
df <- data.frame(
  value = c(rnorm(4000, mean = 0, sd = 8.4), rnorm(4000, mean = 0, sd = 0.2)),
  type = rep(c("Weak", "Informative"), each = 4000)
)

ggplot(df, aes(x = value, fill = type)) +
  geom_density(alpha = 0.5) +  # 透明度を設定
  scale_fill_manual(values = c("skyblue", "salmon")) +  # 色を指定
  labs(title = "Comparison of Priors", x = "Value", y = "Density") +
  theme_minimal()

```

```{r, echo=FALSE}
res_prior <- rstanarm::stan_glm(        
         mpg ~ cyl, 
         seed = 123,
         data = mtcars,
         refresh = 0,
         prior = normal(0, 0.2) # 事前分布設定
         ) 

summary(res_prior)
```

- 異なる事前分布を持つ回帰モデルから得られた事後分布の描画

  - より狭い事前分布をおくと、事前分布が推定に与える影響が、広い事前分布の場合より大きくなる。
  
    - 広い、狭い = 標準偏差の大きさ
  
```{r, echo=FALSE}
posterior_sample <- as.matrix(res, pars = "cyl")
df_posterior <- data.frame(value = posterior_sample, type = "Posterior (Weak)")
posterior_sample2 <- as.matrix(res_prior, pars = "cyl")
df_posterior2 <- data.frame(value = posterior_sample2, type = "Posterior (Informative)")

df_combined <- rbind(df_posterior, df_posterior2)

ggplot(df_combined, aes(x = cyl, fill = type)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Posterior Distribution",
       x = "Coefficient Value",
       y = "Density") +
  geom_vline(xintercept = 0, lwd  = 2) +
    scale_fill_manual(values = c("skyblue", "salmon"))   # 色を指定

```

<div class="alert alert-info">
  <strong>注</strong> 事前分布を狭く指定しない場合、頻度統計の結果もベイズ統計の結果も近似する。
</div>

- ベイズ推定では、事前分布の設定を行ったり、サンプリングが適切に行われたかを確認する手順が必要である。今回は推定の紹介だけで、今後詳しい設定法などに言及する。

:::

### 回帰分析における予測値と残差
- **残差（residual）**：予測の誤差ともいう。それぞれの観測値と直線の差のこと。つまり、\(e = y - \hat{y}\) 。

  - 残差の平均は0
  - 残差と変数 *x* の相関は0
  - 予測値 \(\hat{y}\) と残差の相関は0
  
    - 予測値は変数 *x* を線形下もので、相関係数の絶対値は変わらないから
  
- 前のセクションで述べた **最小2乗法**は残差を2乗して足し合わせた残差平方和が最小になるように計算を行う

#### デモ
- HPを従属変数、こうげきを独立変数として単回帰分析を実行し、resという変数に格納する
```{r}
res <- lm(dat$HP ~ dat$こうげき)
```

```{r, echo=FALSE}
summary(res)
```


- 残差の平均を計算
```{r}
mean(res$residuals)
```

- 残差と変数 *x* の相関を計算
```{r}
cor(res$residuals, dat$こうげき)
```

- 残差と予測値の相関を計算
```{r}
cor(res$residuals, res$fitted.values)
```

::: infobox
R で計算すると、理論的には 0 になるはずの値が**浮動小数点演算の誤差**により完全に 0 にならないことがあります([参考](https://stackoverflow.com/questions/23992032/sum-of-residuals-using-lm-is-non-zero))。
:::

#### 変数の直行分解と残差の意義

- 残差の式は以下のように書き換えられる。

\[
y = \hat{y} + e
\]

- 前のセクションで示したように、右辺の二つは無相関である。従って、従属変数を無相関の成分に分解する式であると言える。相関がない2変数は「互いに直交する」ともいえる。つまり、上記の式は、**直行分解**の式である。

  - 独立変数と残差が無相関であるという性質により、「従属変数 *y*の成分のうち、独立変数 *x* とは相関の無い残差成分」を取り出すことが可能になる。
  
  - つまり、残差は単なるズレではなく、従属変数のうち独立変数とは関係しない部分を表している
  
::: infobox

残差と誤差は異なる。「誤差」は求めようとする**真**の回帰式（母集団などのように神様しか知らない）から算出される値と実際のデータとの差を表す。真の回帰式は理論的なものであるため誤差を計算では求められない。「残差」は実際のデータを用いて推定された回帰式から算出される値と実際のデータとの差を指し、計算で求められる。

:::
  
##### ポケモンのHPとこうげきの例
- ポケモンのHPをこうげき変数で予測する（「こうげき」の値が大きいポケモンはHPも高そう）
  - こうげき：```r (dat$こうげき[384])```
  - HP：```r (dat$HP[384])```
```{r, echo=FALSE}
img0 <- image_read(dat$画像URL[384])
print(img0, info = F)
```

  - 残差は「HPのうち、こうげきでは説明できない成分」
  
    - 残差が**正**の大きな値：「こうげきから予測されるレベルよりもHPがかなり高いポケモン」

```{r, echo=FALSE}
pok <- which.max(res$residuals)
img <- image_read(dat$画像URL[pok])
print(img, info = F)
```

- HPの平均と当該ポケモンのHP
```{r, echo=FALSE}
mean(dat$HP)
dat$HP[pok]
```

- こうげきの平均と当該ポケモンのこうげき
```{r, echo=FALSE}
mean(dat$こうげき)

dat$こうげき[pok]
```

  - 残差が**負**の大きな値：「こうげきから予測されるレベルよりもHPがかなり低いポケモン」

```{r, echo=FALSE}
pok2 <- which.min(res$residuals)
img2 <- image_read(dat$画像URL[pok2])
print(img2, info = F)
```

- HPの平均と当該ポケモンのHP
```{r, echo=FALSE}
mean(dat$HP)

dat$HP[pok2]
```

- こうげきの平均と当該ポケモンのこうげき
```{r, echo=FALSE}
mean(dat$こうげき)

dat$こうげき[pok2]
```

- 残差（変数）はHPそのものとは意味内容が異なる（「こうげきから予測されるHPよりも高いか低いか」を示している）。HPそのものが高いポケモンでも、こうげきが高ければ残差の値は大きくならない。また、HPそのものが低くても、こうげきのわりにはHPが高ければ残差は大きくなる。

  - 残差に注目することで、もともとの変数間の関係を調べるだけでは知ることのできなかったより本質的な関係が明らかになる可能性がある
  
### 回帰直線の当てはまりの良さ

- 相関係数の2乗のことを**分散説明率**とよぶことがある。また、独立変数がどれだけ従属変数の値を決定するかを表していることから、**決定係数（coefficient of determination: \(R^{2}\)）**とも呼ばれる。高いほどよい（高すぎても予測の点から問題はある）。

\[
R^{2} = 1 - \frac{SS_e}{SS_y}
\]

  - \(SS_e\)：残差の2乗和
  - \(SS_y\)：目的変数の偏差2乗和

  - 0-1（0-100%）の間の値をとる

  - 調整済決定係数（Adjusted R-squared）：独立変数の数が多い場合、その影響の大きさに関わらず、決定係数が大きくなる。その欠点を補ったもの


```{r}
summary(res)
```

### 回帰係数の有意性
- 帰無仮説（独立変数の係数が0）を *t* 分布をもとに検定

### 回帰分析を使う際の注意点
- 係数の数値は"Effects"ではなく、"Comparison"

  - 「係数がXXという数値が得られた。従って、ZZ増えると、変数 *y* は上がる/下がる」という主張は、不正確な場合がほとんど。回帰分析は「説明」と「予測」のどちらにも用いられるが、変数 *x* から変数 *y* への→を（「説明」）主張するにはその他の手続きが必要となる。

    - 予測（実用的）：応用的な状況で有用な意思決定を行うために結果や行動の予測する
    - 説明（理論的）：理論の検証や発展のために現象の性質を理解したり説明したりする
    
  - 決定係数の値が高く、よく適合していることと、そのモデル内の回帰係数が「因果効果の良い推定値」かどうかは、本質的には別の問題
  
- 予測変数と目的変数に線形の関係がある

- 十分なサンプルサイズが必要

  - 検定力分析で事前に決定する

- 外れ値がないか

- 残差の独立性があるか

  - 残差に相関がある場合、残差の独立性が満たされていない
  - 時系列など時間に関わるデータはこれが満たされない場合が多い（株価のデータなど）

- 残差の等分散性があるか

  - 残差に何らかの傾向があるとモデルが誤っていると判断する

- 残差が正規分布しているか

   - 回帰モデルは独立変数が正規分布していることを前提としていない。また、従属変数が正規分布していることを仮定しているわけではなく、regression errorが正規分布していることを仮定している。


## ハンズオンセッション

### データの読み込み

- 英語テストの得点が従属変数、独立変数が英語学習歴の長さ


```{r}
dat <- read.csv("../stat_class_2025/sample_data/simp_reg.csv")
```

### 読み込んだデータの確認

```{r}
head(dat, 5)
```
- 記録されていないデータ（セルにNA）がないかを確認する

```{r}
complete.cases(dat)

subset(dat, complete.cases(dat) == FALSE)
```

- NAが入っている行を削除

```{r}
dat.2 <- na.omit(dat)
```

- 削除されているか再度確認
```{r}
subset(dat.2, complete.cases(dat.2) == FALSE)
```
```{r}
summary(dat)
```

- 散布図で外れ値がないかを確認。```par(pty="s")```を実行することで、正方形の図として描画される。

```{r}
par(pty="s")

plot(dat.2$mid, dat.2$end,
     xlim = c(0, 100),
     ylim = c(0, 100),
     xlab = "Mid-term",
     ylab = "Final",
     )
```

- 外れ値
  - データの大部分の傾向と異なるもので、必ずしも誤りとは限らないが、データ集計や分析の際にその存在が結果の精度を悪化させる可能性があるもの。
  - 何を外れ値とするかは研究の目的やデータ収集の状況による。今回はマハラノビスの距離を用いて、行う例を紹介する。

```{r}
d <- mahalanobis(dat.2[,2:3], apply(dat.2[,2:3], 2, mean), cov(dat.2[, 2:3]))
```

```{r}
n <- nrow(dat.2)
v <- ncol(dat.2[, 2:3])

outliers <- n * (n - v) / ((n ^ 2 - 1) * v) * d > qf(0.9, n, v)
```

```{r}
par(pty="s")

plot(dat.2[, 2:3],
     pch = ifelse(outliers, 16, 21),
     xlim = c(0, 100),
     ylim = c(0, 100),
     xlab = "Mid-term",
     ylab = "Final"
)

```

- 外れ値の除去

```{r}
dat.3 <- dat.2[-which(outliers == TRUE),]
```

```{r}
par(pty="s")

plot(dat.3[, 2:3],
     xlim = c(0, 100),
     ylim = c(0, 100),
     xlab = "Mid-term",
     ylab = "Final"
)

```

### 変数間の相関の確認

```{r}
cor(x = dat.3$mid, y = dat.3$end)
```

### 回帰分析の実施
- 最小二乗法

```{r}
model.ls <- lm(end ~ mid, data = dat.3)
```

```{r}
summary(model.ls)
```

- 最尤推定法
```{r}
model.mlm <- glm(end ~ mid, data = dat.3)
```

```{r}
summary(model.mlm)
```

- 決定係数
```{r}
r_2 <- 1 - model.mlm$deviance/model.mlm$null.deviance

r_2

performance::r2(model.mlm)
```

- 自由度調整済決定係数

\[
1 - \frac{n-1}{n-p}(1-R^2)
\]
```{r}
n <- nrow(dat.3)
p <- length(coef(model.mlm)) -1 # 切片を抜いた変数の数

1 - ((1 - r_2) * (n - 1)) / (n - p - 1)
```

- ベイズ推定

```{r}
model.b <- stan_glm(end ~ mid, data = dat.3, refresh = 0, seed = 123)
```

```{r}
summary(model.b)
```


### 結果の解釈
```{r}
par(pty="s")

plot(dat.3[, 2:3],
     xlim = c(0, 100),
     ylim = c(0, 100),
     xlab = "Mid-term",
     ylab = "Final"
)

abline(model.mlm)
```

- 信頼区間も可視化

  - 最尤推定法
```{r}
stats::confint(model.ls, level = 0.95)
```

  - ベイズ推定
```{r}
rstanarm::posterior_interval(model.b, prob = 0.95)["mid", ]
```

```{r}
new <- data.frame(mid = seq(1, 100, 1))

pred <- stats::predict(model.mlm, newdata = new, se.fit = T,
                       level = 0.95, type = "response")

confidence <- data.frame(
  fit = pred$fit, 
  lower = pred$fit - 1.96 * pred$se.fit, 
  upper = pred$fit + 1.96 * pred$se.fit
)
```

- 点線が95%信頼区間
```{r}
par(pty="s")

plot(dat.3[, 2:3],
     xlim = c(0, 100),
     ylim = c(0, 100),
     xlab = "Mid-term",
     ylab = "Final"
)

abline(model.ls)

lines(new$mid, confidence[, 2], lty = 3)
lines(new$mid, confidence[, 3], lty = 3)
```

### モデルの診断
- 予測値と残差の散布図（x軸 = 予測値、y軸 = 残差）

  - 点は、点線に対して、ランダムに散らばっていればよい。赤い線は残差を説明する回帰曲線で、点線（残差0の線）と重なっているほど良いモデル。

  - 残差の絶対値が大きいデータフレームの行番号を表示している
  
```{r}
par(pty="s")
plot(model.mlm, which = 1)
```

- Q-Q（quantile-quantile）プロット（x軸 = 標準正規分布の分位点、y軸 = 残差の分位点）

  - 正規分布であれば、点線に重なる
  
    - 残差の絶対値が大きいデータフレームの行番号を表示している

  - わかりやすいアニメーション（[参考](https://qiita.com/kenmatsu4/items/59605dc745707e8701e0)）


```{r}
par(pty="s")
plot(model.mlm, which = 2)
```

- ```acf```関数で残差の独立性を確認

  - 自己相関係数：一つ前のデータとの相関を示す。Lagはデータの数の半分になる
  - 引数に```plot = T```でコレログラムを描画
  
    - ラグ0は係数1になる。
    - 青い点線は帰無仮説「自己相関係数が0」の95%信頼区間。この中に線が収まっていれば、自己相関がないと判断する
    
```{r}
stats::acf(model.mlm$residuals, plot = F)
```

```{r}
stats::acf(model.mlm$residuals, plot = T)
```


- 論文への記載

  中間試験の得点から学期末試験の得点を予測するため、回帰分析を行った。その結果、係数は統計的に有意であった（*b* = 0.73 [0.60, 0.85], *SE* = 0.07, *p* < .001）。係数は分散の57 %を説明していた（\( R^2\) = 56.7 %, 調整 \( R^2\) = 56.2 %）。
  
  - <u>（結果に対し謙虚）</u>**平均して、**中間試験の得点が1点差の学生を比較した際、中間試験の得点が1点高い学生は、低い学生に比べ、期末試験の得点が0.73点高い。

  - <u>（因果関係を匂わせており不正確）</u>中間試験の得点が1点高くなるほど、期末試験の得点は0.73点高くなる
  
## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

- 心理学統計法　放送大学
- 平井 et al. 
- 心理学統計の基礎
- https://stackoverflow.com/questions/23992032/sum-of-residuals-using-lm-is-non-zero
https://takehiko-i-hayashi.hatenablog.com/entry/2017/09/27/105559
- https://bellcurve.jp/statistics/course/9704.html
- 言葉と数式で理解する多変量解析入門
- https://oroshi.me/2021/01/lsm
- http://www.ner.takushoku-u.ac.jp/masano/class_material/waseda/keiryo/R34_MLE.html#1_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95%E3%81%A8%E6%9C%80%E5%B0%A4%E6%B3%95%E3%81%AE%E9%81%95%E3%81%84
- Gelman et al., Regression and other stories
- https://zenn.dev/tatamiya/articles/0d9a79260ebb42#lm-%E9%96%A2%E6%95%B0%E3%81%AE%E5%AE%9F%E8%A3%85%E3%82%92%E3%81%A9%E3%81%86%E3%82%84%E3%81%A3%E3%81%A6%E8%BE%BF%E3%82%8B%E3%81%8B%EF%BC%9F
- Rによる教育データ分析入門
- https://www.stat.go.jp/training/2kenkyu/ihou/72/pdf/2-2-723.pdf
- https://hira-labo.com/archives/1806
- https://qiita.com/kenmatsu4/items/59605dc745707e8701e0
- 心理学的研究における重回帰分析の適用に関わる諸問題

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:06-regression.Rmd-->

# Week7: 重回帰分析

## メモ
- 重回帰分析の係数の解釈にきを付ける。ある変数の影響を0にした際の影響であることを示す

## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 

## 重回帰分析とは
- 単回帰分析：変数が1つ

\[
\hat{y} = a + bx
\]

- 重回帰分析：変数が2つ以上

\[
\hat{y} = a + {b_1}{x_1} + {b_2}{x_2} + {b_3}{x_3}
\]

### 偏回帰係数
- 重回帰式における回帰係数：偏回帰係数（partial regression coefficient）

  - **他の独立変数の影響を取り除いた（一定にした）**、ある独立変数の従属変数への影響力
  
    - **平均化された 1 単位の比較**

    - e.g., "When comparing two children whose mother have the same level of education, the child whose mother is *x* IQ points higher is predicted to have a test score is 6*x* higher, **on average.** (Gelman et al., 2020, pp. 134)
  
- 変数間で単位が異なる場合、従属変数、独立変数をすべて標準化することで、**標準化偏回帰係数（standardized partial regression coefficient）**を算出し、変数間の影響力を比較する

- （標準化）偏回帰係数は、従属変数と独立変数の**因果関係**を示すわけではないことに注意

### 重相関係数と決定係数

- 重相関係数（multiple correlation coefficient）

  - 全ての独立変数から得られた、従属変数との相関を表す指標
  
    - <u>単回帰の場合は、相関係数の2乗が決定係数</u>

- 調整済み決定係数（Adjusted R-squared）

  - 独立変数の数が多いほど、重相関係数の値が大きくなる（インフレする）ため、修正を加えた値

### 独立変数の有意性
- 個々の独立変数が従属変数の予測に有意に寄与するかは *t* 分布を用いて検定する

  - 帰無仮説：ある独立変数の係数は0

### 回帰式の有意性（ *F* 検定）
- **すべての**偏回帰係数は0であるという帰無仮説を立て、 *F* 値をもとに検定を行う

  - *F* 値（分散比）：モデルの分散を自由度で割った値（平均平方和）÷ モデルの誤差分散を自由度で割った値（平均誤差分散）

- *F* 検定が有意でない場合、モデルからの予測値は実際の値と大きく異なることを示す

## 重回帰分析を行う上での注意点

### サンプルサイズ
- 明確な基準はなく、目標とする効果量や実験項目の数などによって異なる

  - 検定力分析を行い、実験や調査を行う前に事前に決定することが望まれる
  
### 多重共線性（multicolinearity）

- 独立変数間の相関関係が高いと、偏回帰係数を正しく推定することができない

  注意点 (Multicollinearity should not be confused with a raw strong correlation between predictors. What matters is the association between one or more predictor variables, conditional on the other variables in the model.) From (https://rdrr.io/cran/performance/man/check_collinearity.html)

- 多重共線性の問題がないかを確認するための3指標。分析前に閾値を設定し、論文内でも多重共線性の判断をどのようにするか明記しておくとよい

  - 許容度（tolerance）：値が 0.1以下で多重共線性があると判断される
  - VIF（variance inflation factor）： 論文でよく見る指標。許容度の逆数で、5-10以上だと多重共線性だと判断。
  - 条件指数（condition index）：15以上で強い多重共線性があると判断

### 外れ値
- 回帰直線は外れ値に大きく影響する

- 外れ値を調べる代表的な4指標

  - 残差：各データの残差を標準値に変換し、± 2標準偏差もしくは ± 3以上の割合を調べる
  
  - クックの距離：データが回帰式全体に与える影響を示す指標。1以上で問題ありと判断される
  
  - てこ比：各ケースにおける複数の変数データが全体の平均からどの程度ずれているかの指標。平均てこ比の3倍以上の値を取る場合問題ありと判断される。

    - 平均てこ比：独立変数の数 + 1 ÷ サンプルサイズ

  - マハラノビス距離：複数の独立変数における各データの平均と各ケースのデータの距離を示す指標。マハラノビスの表を参考に判断する（マハラノビスとてこ比を両方使う必要はない）。

### 残差の独立性、正規性、等分散、線形性

- 残差の独立性：どの独立変数の残差間にも相関がないという前提。

- 残差の正規性：残差の散布図やヒストグラムなどで確認できる

- 残差の等分散性：独立変数がどの値の場合でも残差分散が同じである必要がある

- 残差の線形性：残差と予測値には線形関係がある必要がある。散布図などで確認

- https://www.note.kanekoshobo.co.jp/n/n9624e14dda8e#733dd8a6-e3c2-4767-8e8b-7a50ae6eeaf4
- https://www.jstage.jst.go.jp/article/jjpsy/92/3/92_92.19226/_pdf


## 投入方
- 重回帰分析の場合、独立変数を入れる順序で書く独立変数の有意性や偏回帰係数が変化する。
- どの方法を用いるのかは研究の目的によっても異なる。基本的に、分析の前に決めておく方がよい。

### 強制投入法
- 理論や仮説に基づいて慎重に選んだすべての独立変数を1度に含めてモデルを作成する。

  - 独立変数を増やすほど決定係数は大きくなることに注意

### 階層的投入法
- 階層的回帰分析とも言われる。理論や仮説に基づいて、独立変数を1つずつ投入していく。段階的にモデルに含めるため、各独立変数がどの程度決定係数の向上に影響を与えているか把握できる（強制投入法のあとに階層的投入法を行うなど）。

### ステップワイズ法
- 統計的回帰分析とも呼ばれる。統計的に最も予測率が高いと考えられる変数から順に自動的に投入される。従属変数と相関の高い独立変数が投入され、その後偏回帰係数の有意性が次に最も高くなる独立変数が選ばれ順に投入される。

  - 階層的投入法では、分析者が投入する順番を決めるが、ステップワイズ法では予測率に応じて自動で投入される。従ってその結果が理論や仮説に基づいて選ばれたモデルになるとは限らない
  
### 変数減少法
- すべての独立変数を投入し、予測への寄与が小さい独立変数から順に変数を抜いていく

## ハンズオンセッション
- 強制投入法、ステップワイズ法のやり方を確認

### データの読み込み
```{r}
library(readr)
dat <- read_csv("../stat_class_2025/sample_data/regression_data.csv")
```

- 必ず中身を確認。
```{r}
head(dat)
```

### 基本統計量の算出

```{r}
library(psych)
describe(dat)
```

### データの図示

- 箱ひげ図
```{r}
boxplot(dat[, 2:6], 
        xlab = "Test",
        ylab = "Score",
        ylim = c(0, 100) # y軸を0から100の範囲で表示する
        )
```

- 蜂群図

```{r}
#install.packages("beeswarm", dependencies = T) #一度やればOK
library(beeswarm)
beeswarm(dat[, 2:6])
```

- 箱ひげ図 + 蜂群図

```{r}
boxplot(dat[, 2:6], xlab = "Test", ylab = "Score", ylim = c(0, 100))
beeswarm(dat[, 2:6],
         add = T #箱ひげ図の上に描画すると明示的に記す
         )
```

### 相関関係の確認
- 高い相関関係を示す独立変数はない
```{r}
psych::corr.test(dat[, 2:6])
```

```{r}
psych::pairs.panels(dat[, 2:6])
```

### 重回帰分析の実施（強制投入法）
- ```lm()```関数を使用する。

- 重回帰分析の結果を```output_forced```という変数に格納する

```{r}
output_forced <- lm(
    EndT ~                                # 従属変数
    Placement + FirstT + MidT + LastT,   # 独立変数
    data = dat                           # データセット
  )
```

- ```summary()```で結果を確認
  - プレイスメントテストの係数が有意ではなく、年度末模試の予測に適さない可能性が示唆された。

  - Adjusted R-squaredの結果を確認すると、4つの独立変数で従属変数の77.6%を説明
  
```{r}
summary(output_forced)
```

### 外れ値の診断
- 分析結果から```resid()```関数で残差を算出し、その値を```scale()```関数で標準化
```{r}
res <- resid(output_forced)

z.res <- scale(res)
```

```{r}
boxplot(z.res)
```
- 最大値、最小値の2つが外れ値の可能性あり

```{r}
describe(z.res)
```

- ```olsrr```関数を使うと、様々な図を一度に表示可能

```{r}
#install.packages("olsrr")
library(olsrr)
ols_plot_diagnostics(output_forced)
```

### 多重共線性の確認
- 多重共線性の問題はなさそう

  - VIF: 5以上のものはない
  - 許容度：0.1以下のものもない
  - 条件指数（Condirion Index）：全て15以下
```{r}
ols_coll_diag(output_forced)
```

### 重回帰分析の実施（ステップワイズ法）
- ```step()```関数を用いる。この関数では、AIC（Akaike's Information Criterion）の値が最も低い（= 最も良いモデル）が選択される。

  - AICが最も小さくなる変数をモデルから削除していった結果が出力される
  
  - -変数名：その変数を抜いたモデルの結果
  - none：すべての変数を含めたモデル
```{r}
output_step <- stats::step(output_forced)
```

- Placementを抜いたモデルの結果

  - 中間試験のみで約71%のデータを説明している（0.71 × 100）
```{r}
summary(output_step)
```


### 結果の報告の仕方
プレイスメントテスト、前期試験、中間模試、および後期試験の得点から年度末試験の得点を予測するために、ステップワイズ法による重回帰分析を行った。その結果、中間模試、後期試験、および前期試験の得点が予測に有意で、この3つの変数のモデルは従属変数の分散の78% ( \(R^2\) = .783.調整 \(R^2\) = .777)を説明しており、かなり予測率が高いといえる。なかでも、中問試験のみによって分散の71%を説明していた。

## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

- 平井 et al. 

- 外国語教育ハンドブック
- 南風原
- https://bellcurve.jp/statistics/course/24461.html?srsltid=AfmBOoq5YejCPBlcGOIawwi-sCV98ib6WvQKuCNEhshmn1IzwdR7JhyV
- Gelman et al., 2021. regression and Other Stories

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:07-multipleregression.Rmd-->

# Week8: 重回帰分析 (2)

## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 

## 分散分析

### *t* 検定覚えてますか？
- 2つのデータの平均値の差を *t* 分布を用いて検定する。

  - e.g., Aグループの英語テストの平均点 vs. Bグループの英語テストの平均点（対応なし）
  
  - e.g., Aグループの英語テストの平均点 vs. Aグループの国語テストの平均点（対応あり）
  
### 検定の繰り返し
- 3つのデータを比較する場合

  - e.g., Aグループの英語テストの平均点 vs. Bグループの英語テストの平均点 vs. Cグループの英語テストの平均点
  
  - グループ間の平均点の差を検定したい
  
    - A vs. B
    - A vs. C
    - B vs. C
    
- 検定における有意水準

  - 検定結果は、5 %（有意水準 5%と設定する場合）の間違いの可能性を含む（第一種の誤り）

    - 帰無仮説が真なのに、誤って偽だと主張すること（本当は差がないのに差があると判断する）
  
```{r, echo = F, fig.cap = "image source: https://www.statisticssolutions.com/to-err-is-human-what-are-type-i-and-ii-errors/"}
frink <- magick::image_read("C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png")
magick::image_write(frink, "C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png", format = "png")
knitr::include_graphics("C:/Users/terai-masato/Documents/stat_class_2025/picture/error.png")
```

- 検定を繰り返すことの問題

  - 第一種の誤りの確率が当初設定した数値よりも大きくなってしまう
  
  - 同時に帰無仮説が2つ以上成立する場合に問題となる
  
    - 上記の例では帰無仮説（「AとBに差はない」 + 「AとCに差はない」 + 「BとCに差はない」）のいずれか一つが棄却されればよい。
    
    - 1回の検定で帰無仮説が保留される確率 = 95 %（1 - 0.05 (有意水準)）

    - 3回の検定で全て保留される確率 = 0.95 × 0.95 × 0.95 = 0.857 = 86 % 
    
    - 3回の中でいずれか一つが棄却される確率
    
      - 1 - 0.857 = 0.142 = 約 14%
      
      - 当初の値の3倍近くまでエラーが膨張！

  - 同じデータに対して複数回検定を行うこともエラー率を高める
  
    - 留学から帰ってきた学生に質問紙（100項目）を配布し、留学前と留学後でどの項目に差がみられるか確認する

\[
1 - (1 - 0.05)^{100}
= 99.4 %
\] 
    - 同じ参加者のリーディング、リスニング、スピーキングテストそれぞれに2群の *t* 検定
    
::: infobox
「AとBの比較を行う。正規性の確認のために正規分布かを検定し、その後、等分散性を検定でさらに確認し、 *t* 検定を行った。」も検定の多重性の問題に該当する。この場合、AもBのデータもそれぞれ正規分布に従うという二つの帰無仮説を成立させる必要がある。またいくつかの検定を通過させ、条件（都合）に合う検定を選ぶことにもなる。
:::

### 検定の多重性を考慮する

- 多重比較補正を行う

  - 有意水準を調整する

    - ボンフェロー二補正、ホルム補正など
    
  - ボンフェロー二補正
  
    - 有意水準の値を厳しくする
    
    - 3回繰り返す場合、毎回の有意差を 0.05 / 3 = 0.017
    
- 新しいサンプルを集める

  - e.g., リーディングテストとは別のサンプルを集めて、リスニングテストの検定を行う

- 3群以上の比較が可能な分析を使う

  - **分散分析**など

## 分散分析と回帰分析の関係
- 前の週では、量的変数による重回帰を扱った。しかし、重回帰分析では、質的変数（e.g., 性別、クラス）同士、量的変数と質的変数などのデータを扱う場合もある。

  - 独立変数が一つ：単回帰分析
  
  - 独立変数が二つ以上：重回帰分析
  
  - 独立変数が質的（独立変数の数は関係ない）：分散分析
  
    - 分散分析に、量的変数（共変量）を加える：共分散分析

## 分散分析は今回扱いません

### 理由 1
- 当該分野で分散分析を行っている論文をほとんど見なくなったから

  - 2010年代前半までの研究では分散分析が多いです。

  - 僕は授業以外で分散分析をやったことがありません

### 理由 2
- 分散分析よりも自由に分析が行える（理由1に関連している）

  - 分散分析は独立変数として質的変数を仮定するが、実際には量的変数との交互作用を検討したい場面が多い
  
    - 過去の研究を見ると、量的変数を質的変数に変換する手立てが取られたりした（語彙サイズを得点をもとに、閾値を求めて低・中・高にしたり）
  
  - 後半で扱う一般化線形混合効果モデルでは、様々な確率分布、ランダムな誤差を考慮できる
  
  - 分散分析と同じような平均値の比較を行うことが可能
  
    - 質的変数のコーディング
  
::: infobox
分散分析は統計学の教科書であれば必ず登場します。それほど学習する必要性のある内容で、今後説明する回帰モデルでの質的変数の扱いでも、分散分析の考え方を理解しておくと理解しやすいです。しかし、本講義は統計学以外にプログラミング言語の習得まで扱うため、時間的制約があります。そのため、やむを得ずシラバスから削除しました(´;ω;｀)。自分が使わなくても以前の研究を読む際に必要になることもあり得ますよね。
:::


## 重回帰分析での質的変数の扱い方

### 変数のコーディング
- 指導法A、Bなどの変数はそのまま分析することができない。その為、数値のデータをあてはめて分析を行う（データを変換する）。この手立てを変数を△△コーディングするという。

  - 今回扱うコーディングにより、分析結果の係数の数値の意味が異なるため、重回帰分析の肝となる内容である。
  
- 言語研究で使われる頻度が高いコーディング法

```{r}
set.seed(123)  # 再現性のための乱数設定

# サンプルサイズ
n <- 20

# 疑似データの作成
df <- data.frame(
  ID = 1:n,
  Method = factor(rep(c("A", "B", "C"), length.out = n)),  # 3つの指導法を割り当て
  Score = round(runif(n, min = 50, max = 100), 1)  # 50-100の範囲でランダムに得点を生成
)

# データの表示
print(df)

```


  - トリートメント・コントラスト（treatment contrasts）

    - 別名ダミーコーディング
    
    - **基準のグループと、それ以外の各グループを比較**
    
    - ```contr.treatment(水準数)```関数（```stats```パッケージ）
    
```{r}
library(stats)
contrasts(df$Method) <- contr.treatment(3) 
```

- 指導法Aが基準となる（0）

  - 指導法AとB、指導法AとCの比較
```{r}
df$Method
```

- 係数の方向（正/負）も重要

  - 切片：基準の指導法Aの平均値
  
  - Method 2: 指導法B - 指導法A

  - Method 3: 指導法C - 指導法A

```{r}
results <- lm(Score ~ Method, data = df)
results$coefficients
```

```{r}
# 各指導法の平均を確認
aggregate(Score ~ Method, data = df, FUN = mean)
```

  - サム・コントラスト（sum contrasts）
  
    - **全グループの平均の平均値（GM: grand mean）と1が割り当てられたデータを比較**

    - ```contr.sum(水準数)```関数（```stats```パッケージ）

```{r}
library(stats)
contrasts(df$Method) <- contr.sum(3) 
```

- 指導法Cが比較から除外（0）

  - 指導法Aと全体平均、指導法Bと全体平均の比較
```{r}
df$Method
```

- 係数の方向（正/負）も重要

  - 切片： GM
  
  - Method 1: 指導法A - GM

  - Method 2: 指導法B - GM

```{r}
results_sum <- lm(Score ~ Method, data = df)
results_sum$coefficients
```

```{r}
tmp <- aggregate(Score ~ Method, data = df, FUN = mean)

GM <- mean(tmp$Score)

GM
```

  - 反復コントラスト（repeated contrasts）

    - **隣接する2つのグループを比較する**
    
    - 上・中・下の場合、上 vs. 中、中 vs. 下

    - ```contr.sdif(水準数)```関数（```MASS```パッケージ）

```{r}
library(MASS)
contrasts(df$Method) <- contr.sdif(3) 
```

- 指導法Aと指導法B、指導法Bと指導法Cの比較
```{r}
df$Method
```

- 分数表示
```{r}
contrasts(df$Method) <- fractions(contr.sdif(3))
df$Method
```


- 係数の方向（正/負）も重要

  - 切片： GM
  
  - Method 2-1: 指導法B - 指導法A

  - Method 3-2: 指導法C - 指導法B

```{r}
results_rep <- lm(Score ~ Method, data = df)
results_rep$coefficients
```

- 全体のコーディングの比較

```{r, echo=FALSE}
sjPlot::tab_model(results, results_sum, results_rep, show.ci = F, dv.labels = c("Treatment", "Sum","Repeat"))
```


- 他にも多項コントラスト、ヘルムートコントラストなど

- 変数の順番を並び替える方法

  - ```factor()```関数を使う

```{r}
df$Method <- factor(df$Method, 
                    levels = c("C", "A", "B"))
```

::: infobox
- 研究課題に合わせてコーディング法を選ぶ必要があり、またどのコーディングを使用したか、どの数値をどのデータにあてはめたのかを報告する必要がある。
- 参加者内/参加者間のデータかは関係ない。
- 以上のどのコーディング法でも、水準数 - 1が計算される
:::


## ハンズオン・セッション
- ポケモンのデータセットの中身を減らしたもの

```{r}
library(readr)
dat <- read_csv("../stat_class_2025/sample_data/week8_data.csv")
```

### データの概要

- 今回は、重さ、タイプ1、世代の三つに関心があると仮定する

- データ構造の分かりやすさを優先しており、モデル自体の妥当性は考慮していないことに注意

```{r}
head(dat, 5)
```

```{r}
library(psych)

describe(dat$重さ)
```
```{r}
boxplot(dat$重さ)
```


```{r}
table(dat$タイプ1)
```

```{r}
boxplot(dat$重さ~ dat$タイプ1)
```

```{r}
aggregate(重さ ~ タイプ1, data = dat, FUN = mean)
```

```{r}
table(dat$世代)
```

```{r}
boxplot(dat$重さ~ dat$世代)
```

```{r}
aggregate(重さ ~ 世代, data = dat, FUN = mean)
```


### 2水準のコーディング
- 世代間の重さの比較をトリートメントコントラストコーディングで検討

- トリートメントコントラスト

  - ノーマルタイプを基準にする

- 水準の順番を確認
```{r}
factor(dat$タイプ1)
```

- コーディングする際は、**Factor型**になっている必要がある

```{r}
class(dat$タイプ1)
```

- Factor型に変更する
```{r}
dat$タイプ1 <- factor(dat$タイプ1)
class(dat$タイプ1)
```

- トリートメントコントラストコーディングを実施
```{r}
contrasts(dat$タイプ1) <- contr.treatment(2)
```

- ノーマルが基準となっている
```{r}
contrasts(dat$タイプ1)
```

- (単)回帰分析の実施

```{r}
results_tr <- lm(重さ ~ タイプ1, data = dat) 
```

- みずタイプポケモンの方が、平均して42.56kg ノーマルポケモンよりも重いこと。
```{r}
summary(results_tr)
```

- 報告の例

ポケモンの重さを、ポケモンのタイプでどの程度予測できるかを調査するため、回帰分析を行った。その結果、ポケモンのタイプは重さを統計的有意に予測していた。しかし、決定係数の値は8.8%、調整済決定係数は6.7%と予測力は大きくなかった。トリートメントコントラストコーディングでノーマルタイプを基準に（ノーマル = 0、みず = 1）比較すると、みずタイプのポケモンはノーマルタイプのポケモンよりも42.56 kg平均して重いことが予想される。

### 3水準のコーディング
- 反復コントラストコーディングで世代間の重さを比較

```{r}
dat$世代 <- factor(dat$世代)
```

```{r}
contrasts(dat$世代) <- fractions(contr.sdif(3))
```

```{r}
contrasts(dat$世代)
```

- （単）回帰分析
```{r}
results_repe <- lm(重さ ~ 世代, data = dat)
summary(results_repe)
```

- 世代1と3を比較するためコーディングを変更

```{r}
dat$世代 <- factor(dat$世代, levels = c("1", "3", "2"))

contrasts(dat$世代)
```

```{r}
contrasts(dat$世代) <- fractions(contr.sdif(3))
```

- 世代2-1：世代3 - 世代1

- 世代3-2：世代2 - 世代3
```{r}
results_repe2 <- lm(重さ ~ 世代, data = dat)
summary(results_repe2)
```

- 報告の例

ポケモンの重さを、ポケモンの世代でどの程度予測できるかを調査するため、回帰分析を行った。その結果、ポケモンの世代は重さを統計的有意に予測していなかった。しかし、決定係数の値は0.42%、調整済決定係数はマイナスの値を示していた。さらに、*F* 検定に有意差は見られなかった。世代間の比較においても、有意差がみられるペアはなかった。

## 検定の繰り返し問題アゲイン

- *t* 検定から分散分析の接続で取り上げたりしますが、 *t* 検定に限った話ではなく、これから先の分析でも、**帰無仮説と付き合うのであれば**、向き合っていく必要のあるテーマです。常に意識しましょう。

### 重回帰分析における検定

- 検定の多重性は、第一種の過誤を高めてしまうことが問題

- 重回帰分析で使用される検定として二つある（モデル比較の際にはさらに検定を行う）

  - モデル全体: *F* 検定
  
    - 回帰モデル自体の有意性を検定。帰無仮説「すべての偏回帰係数がゼロ」

- 各係数

  - 各係数は *t* 値をもとに検定が行われている
  
    - 2水準と3水準では、係数の数がそれぞれ1と2となり、水準が増えるほど検定を繰り返すことになる

::: infobox
同じ参加者に対し、英語と日本語で実験を行い、言語別のデータセットを作成。それらに重回帰分析を行うと、検定を繰り返していることになります。この場合、モデルに言語変数を入れ、下位検定で多重比較を行って言語ごとの影響を確認します。
:::

::: infobox
各係数での検定の繰り返しの問題は、この分野ではあまり問題にされることが少ないと思います。しかし、シミュレーションを行うと、係数の *p* 値がインフレするのを確認できるのも事実です。
:::

## 解決策

### *p* 値の補正

- ```p_adjust()```関数

  - ```lm()```関数で作成したモデルを使用する。
  -  補正法： Romano-Wolf stepdown（初期値）, Bonferroni, Bonferroni-Holm, and Benjamini-Hochberg corrections, etc...

```{r}
#install.packages("hdm")
library(hdm)
results_sum_RM <- p_adjust(results_sum, method = "bonferroni")
```

- 補正後
```{r}
results_sum_RM[,2]
```

- 補正前
```{r}
summary(results_sum)[["coefficients"]][,4]
```

- 補正では第2種の誤りが大きくなるのを防げないというシミュレーション結果もある

### ベイズ統計
- 検定の繰り返しは、帰無仮説検定を行う頻度統計における問題。

  - 以前使用した```brm()```関数を使用。

```{r}
contrasts(dat$タイプ1) <- contr.sum(2)
```


```{r}
library(brms)

results_sum_bayes <- brm(
  重さ ~ タイプ1,  # 応答変数「重さ」と説明変数「タイプ1」の回帰モデル
  data = dat,  # データフレーム dat を使用
  family = gaussian(),  # 正規分布（ガウス分布）を仮定
  prior = prior(normal(0, 10)),  # 事前分布として平均0、標準偏差10の正規分布を設定
  chains = 4,  # MCMCの独立した4つのチェーンを使用
  iter = 5000,  # 総サンプリング回数（各チェーンあたり5000回）
  warmup = 500,  # ウォームアップ（バーンイン）期間として最初の500回を破棄
  thin = 2,  # 2回ごとに1つのサンプルを取得（間引き）
  seed = 123,  # 乱数シードを設定し、再現性を確保
  refresh = 0
)

```

```{r}
summary(results_sum_bayes)
```


## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. Week 7の内容も踏まえ、以下のデータを基に重回帰分析を行いましょう

  - 外れ値のデータが入っていることに注意です

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで


## 参考文献

- 平井 et al. 

- 外国語教育ハンドブック
- 南風原
- https://bellcurve.jp/statistics/course/24461.html?srsltid=AfmBOoq5YejCPBlcGOIawwi-sCV98ib6WvQKuCNEhshmn1IzwdR7JhyV
- https://home.hirosaki-u.ac.jp/pteiki/r/3pecification/multiplecomp/
- Plonsky, L., & Oswald, F. L. (2017). Multiple regression as a flexible alternative to ANOVA in L2 research. Studies in Second Language Acquisition, 39(3), 579-592.
- 水本篤. (2009). 複数の項目やテストにおける検定の多重性: モンテカルロ・シミュレーションによる検証. 外国語教育メディア学会機関誌, 46, 1-19.
- https://yukiyanai.github.io/jp/classes/econometrics1/contents/R/multiple-comparison.html#%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BC%83%E8%A3%9C%E6%AD%A3%E3%81%AE%E8%A8%88%E7%AE%97%E6%B3%95
- 小島ますみ（2022）. 外国語教育研究における（一般化）線形混合モデル：仮説に適したコーディング・モデリングを中心に The 2021 Annual Conference on Vocabulary Acquisition

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:08-multiplereg_2.Rmd-->

# Week9: 重回帰分析 (3)：交互作用の解釈

```{r, include=FALSE}
library(rstan)
library(brms)
library(gt)
library(tidyverse)

rstan_options(auto_write = T)
options(mc.cores = parallel::detectCores())
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 

## 交互作用
- 2つ以上の独立変数の組み合わせが従属変数にもたらす影響

  - 質的変数は**トリートメント・コーディング**を行ったと仮定
  
  - 一般化線形モデルでは、「説明変数同士の積」を説明変数として加えることで、交互作用を表現する

```{r, include=FALSE}
dat <- read.csv("../stat_class_2025/sample_data/3-10-3-interaction-3.csv")
```

### 量的変数 × 量的変数
```{r}
model1 <- brm(sales ~ product * clerk, gaussian(link = "identity"),
              data = dat, seed = 123, refresh = 0, iter = 5000, chains = 2)
```

```{r, include=FALSE}
eff <- conditional_effects(model1, effects = "product:clerk")
```

- 緑色（平均値）の線、赤色の線は右肩上がりになっている

```{r}
plot(eff, plot = F)[[1]]
```

```{r, echo=FALSE}
sjPlot::tab_model(model1)
```

- 量的 × 量的変数の交互作用の予測値

  - 切片 + Product × (-2.26) + Clerk × (6.49) + **（Product × Clerk） × 1.05**
  
  - 店員数（Clerk）によって、製品数（Product）の係数の値が変化する

```{r, echo=FALSE}
new_data <- data.frame(
  product = c(0, 10, 0, 10),
  clerk = c(0, 0, 10, 10)
)

res <- round(fitted(model1, new_data), 2)

cbind(new_data, res) %>%
  gt() %>%
  tab_header(title = "量的 × 量的変数の交互作用") %>%
  cols_align(align = "center") %>%
  tab_options(table.width = pct(100))
```

tab: https://gedevan-aleksizde.github.io/rmarkdown-cookbook/html-tabs.html

### 量的変数 × 質的変数
- データの読み込み
```{r}
dat <- read.csv("../stat_class_2025/sample_data/3-10-2-interaction-2.csv")
```

- コーディング（トリートメント）

  - 他のコーディングでも分析可能である
  
```{r}
dat$publicity <- factor(dat$publicity)
contrasts(dat$publicity) <- contr.treatment(2)
contrasts(dat$publicity)
```

```{r}
model2 <- brm(sales ~ publicity * temperature, gaussian(link = "identity"),
              data = dat, seed = 123, refresh = 0, iter = 5000, chains = 2)
```

```{r, include=FALSE}
eff2 <- conditional_effects(model2, effects = "temperature:publicity")
```

- 宣伝アリの方が、切片（線の開始点）も傾きも大きいことが分かる
```{r}
plot(eff2, plot = F)[[1]]
```
```{r, echo=FALSE}
sjPlot::tab_model(model2)
```

- 質的 × 量的変数の交互作用の予測値

```{r}
df <- data.frame(
  宣伝 = c("なし", "あり"),
  `売り上げの予測値` = c(
                 "42.80 + temperature × 2.59",
                 "42.80 + 17.49 + temperature × (2.59 + 4.19)")
)

df %>%
  gt() %>%
  tab_header(title = "質的 × 量的変数の交互作用") %>%
  cols_align(align = "center") %>%
  tab_options(table.width = pct(100))

```

- 気温の主効果は、宣伝がなかった時における気温の効果であることに注意。そのため、気温の主効果の数値だけではなく、交互作用を確認して結果を解釈する必要がある。

```{r, echo=FALSE}
new_data_2 <- data.frame(
  publicity = rep(c("not", "to_implement"), each = 2),
  temperature = c(0, 10, 0, 10)
)

res2 <- round(fitted(model2, new_data_2), 2)

cbind(new_data_2, res2) %>%
  gt() %>%
  tab_header(title = "質的 × 量的変数の交互作用") %>%
  cols_align(align = "center") %>%
  tab_options(table.width = pct(100))
```

### 質的変数 × 質的変数

```{r}
dat <- read.csv("../stat_class_2025/sample_data/3-10-1-interaction-1.csv")
```

- コーディング（トリートメント）
```{r}
dat$publicity <- factor(dat$publicity)
contrasts(dat$publicity) <- contr.treatment(2)
contrasts(dat$publicity)
```

```{r}
dat$bargen <- factor(dat$bargen)
contrasts(dat$bargen) <- contr.treatment(2)
contrasts(dat$bargen)
```

```{r}
model3 <- brm(sales ~ publicity * bargen, gaussian(link = "identity"),
              data = dat, seed = 123, refresh = 0, iter = 5000, chains = 2)
```


```{r, include=FALSE}
eff3 <- conditional_effects(model3, effects = "publicity:bargen")
```

- 宣伝も安売りもありの方が売り上げが最も大きいことが分かる
```{r}
plot(eff3, plot = F)[[1]]
```

```{r, echo=FALSE}
sjPlot::tab_model(model3)
```

- 質的 × 質的変数の交互作用の予測値

  - 宣伝も安売りもあった場合、二つの主効果に加え、交互作用の影響が加算される
  
```{r}
df <- data.frame(
  宣伝 = c("なし", "あり", "なし", "あり"),
  安売り = c("なし", "なし", "あり", "あり"),
  `売り上げの予測値` = c("103.32", "103.32 + 10.05", "103.32 + 27.35", "103.32 + 10.05 + 27.35 + 20.72")
)

df %>%
  gt() %>%
  tab_header(title = "質的 × 質的の交互作用") %>%
  cols_align(align = "center") %>%
  tab_options(table.width = pct(100))

```

- 得られた係数の結果と比較するとさらに分かりやすい
```{r, echo=FALSE}
new_data_3 <- data.frame(
  publicity = rep(c("not", "to_implement"), 2),
  bargen = rep(c("not", "to_implement"), each = 2)
)

res3 <- round(fitted(model3, new_data_3), 2)

cbind(new_data_3, res3) %>%
  gt() %>%
  tab_header(title = "質的 × 質的変数の交互作用") %>%
  cols_align(align = "center") %>%
  tab_options(table.width = pct(100))
```

#### 下位検定
- 交互作用が有意だった場合に、単純主効果を調べるために行う

  - 単純主効果
  
    - ある要因の各水準における、別の要因の効果のこと
  
    - e.g., Publicity条件における、Bargen実施の有無（to_impement/not）の平均値差

- 多重比較

  - 水準が3以上などの場合、検定を繰り返すため、多重比較の問題が発生する。影響を軽減するため、補正を行う
  
- 交互作用が有意
```{r, echo=FALSE}
model4 <- lm(sales ~ publicity * bargen, data = dat)
sjPlot::tab_model(model4)
```

```{r, echo=FALSE}
ggplot(dat, aes(x = publicity, y = sales, group = bargen, color = bargen)) +
  stat_summary(fun = mean, geom = "line", aes(group = bargen)) + 
  stat_summary(fun = mean, geom = "point", size = 3) +  # 平均値の点
  theme_minimal()
```
- ```emmeans()```関数を使って、下位検定を行う

```{r}
#install.packages("emmeans")
library(emmeans)
```

-  bargenの単純主効果（publicityの各水準ごとにbargenの効果を調べる）

  - bargen by publicity

```{r}
contrast(
  emmeans(model4, pairwise ~ bargen | publicity), 
  adjust = "bonferroni"
  )
```

-  publicityの単純主効果（bargenの各水準ごとにpublicityの効果を調べる）

  - publicity by bargen 

```{r}
contrast(
  emmeans(model4, pairwise ~ publicity | bargen), 
  adjust = "bonferroni"
  )
```

## ハンズオンセッション

### 質的 × 質的（サムコントラスト）

```{r}
dat <- read.csv("../stat_class_2025/sample_data/Example4.csv")
```

```{r}
head(dat, 5)
```

```{r}
table(dat$Grade)
```

```{r}
table(dat$Learning)
```

```{r}
dat$Grade <- factor(dat$Grade)
dat$Learning <- factor(dat$Learning, levels = c("List", "Context", "KeyWord"))
```

- 各群ごとの平均
```{r}
aggregate(Post ~ Grade, data = dat, FUN = mean)
```

```{r}
aggregate(Post ~ Learning, data = dat, FUN = mean)
```

```{r}
res <- aggregate(Post ~ Grade*Learning , data = dat, FUN = mean)
res
```

- Ground Mean
```{r}
mean(res$Post)
```

```{r}
contrasts(dat$Grade) <- contr.sum(2)
contrasts(dat$Grade)
```

```{r}
contrasts(dat$Learning) <- contr.sum(3)
contrasts(dat$Learning)
```

```{r}
model_sum <- lm(Post ~ Grade * Learning, data = dat)
```

```{r}
summary(model_sum)
```

- 以下のような作図を行う場合、コーディングによって図中のパターンが変わることはない
```{r}
#install.packages("sjPlot")
library(sjPlot)
plot_model(
  model_sum, 
  type = "pred",
  terms = c("Grade", "Learning")
           )
```

### 係数の解釈
- 記述統計の値を参照した考えると分かりやすい
  - Intercept: grand mean (47.5)
  - GradeSecond: First - grand mean (39.93333 - 47.5)
  - Learning1: List - grand mean (65.1 - 47.5)
  - Learning2: Context - grand mean (32.5 - 47.5)
  - GradeSecond:Learning1: {2 * (60 - 70.2) + (45.2 - 19.8) + (49.8 - 40)} ÷ 6
  
$$
\frac{2(M_{\text{リスト1}} - M_{\text{リスト2}}) + (M_{\text{文脈2}} - M_{\text{文脈1}}) + (M_{\text{キー2}} - M_{\text{キー1}})}{6}
$$

  - GradeSecond:Learning2: {(70.2-60) + 2 * (19.8-45.2) + (49.8 - 40)} ÷ 6

$$
\frac{(M_{\text{リスト2}} - M_{\text{リスト1}}) + 2(M_{\text{文脈1}} - M_{\text{文脈2}}) + (M_{\text{キー2}} - M_{\text{キー1}})}{6}
$$

### 質的 × 質的（反復コントラスト）

```{r}
library(MASS)
contrasts(dat$Grade) <- fractions(contr.sdif(2))
contrasts(dat$Grade)
```

```{r}
contrasts(dat$Learning) <- fractions(contr.sdif(3))
contrasts(dat$Learning)
```

```{r}
model_rep <- lm(Post ~ Grade * Learning, data = dat)
```

```{r}
summary(model_rep)
```

### 係数の解釈
- 記述統計の値を参照した考えると分かりやすい
  - Intercept: grand mean
  - Grade2-1: Second - First
  - Learning2-1: Context - List
  - Learning3-2: Keyword - Context
  - Grade2-1:Learning2-1: (ContextにおけるSecond-First) -  (ListにおけるSecond-First) = (45.2 - 19.8) - (70.2 - 60)
  - Grade2-1:Learning3-2: (KeywordにおけるSecond-First) -  (ContextにおけるSecond-First) = (49.8 - 40) - (45.2 - 19.8)


- ```emmeans()```関数で下位検定を行う場合も、どのコーディングをあてはめても結果は同じになります。

```{r, include=FALSE, eval=FALSE}
library(emmeans)

# サンプルデータの作成
set.seed(123)
df <- data.frame(
  class = rep(c("X", "Y"), each = 15),  # 2つのクラス
  group = rep(c("A", "B", "C"), each = 5, times = 2),  # 3つのグループ
  score = c(
    rnorm(5, mean = 5, sd = 1), rnorm(5, mean = 7, sd = 1), rnorm(5, mean = 9, sd = 1),  # class X
    rnorm(5, mean = 6, sd = 1), rnorm(5, mean = 6.5, sd = 1), rnorm(5, mean = 7, sd = 1)  # class Y
  )
)

df$class <- factor(df$class)
df$group <- factor(df$group)

# Treatment contrast (デフォルト)
contrasts(df$class) <- contr.treatment(2)
contrasts(df$group) <- contr.treatment(3)

# Sum contrast
contrasts(df$class) <- contr.sum(2)
contrasts(df$group) <- contr.sum(3)

# Repeated contrast (MASS パッケージが必要)
library(MASS)
contrasts(df$class) <- contr.sdif(2)
contrasts(df$group) <- contr.sdif(3)

model <- lm(score ~ class * group, data = df)

# emmeans の適用
contrast(
  emmeans(model, pairwise ~ class | group), 
  adjust = "bonferroni"
)
```


::: infobox
- ここまで見てきたように、どのようなコーディングを適用したかで係数の値は異なります。また、何と何を比較しているのかによって係数の正負も変わります。必ず、どの変数にどのようなコーディングを適用したのか、どの水準とどの水準を比較したのかを第三者から見て分かるように記載しましょう。
- 2水準 × 3水準の交互作用でも係数の解釈は結構複雑になります。デザインはシンプルな方が解釈する側も分析をしやすいです。
:::

## 最後に

- 清水先生の「心理学における重回帰分析の使い所を考える」に関してのコラムをいれる」


## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）- 外れ値を考えてやる

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで


## 参考文献

- 南風原
- 馬場 RとStanではじめるベイズ統計モデリングによるデータ分析
- 小島ますみ（2022）. 外国語教育研究における（一般化）線形混合モデル：仮説に適したコーディング・モデリングを中心に The 2021 Annual Conference on Vocabulary Acquisition
- https://debruine.github.io/faux/articles/contrasts.html#x3-design

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:09-multiplereg_3.Rmd-->

# Week10: 一般化線形モデル：ロジスティック回帰分析

```{r, include=FALSE}
library(rstan)
library(brms)
library(gt)
library(tidyverse)
library(kableExtra)

rstan_options(auto_write = T)
options(mc.cores = parallel::detectCores())
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 

## 一般化線形モデル
- 確率分布に正規分布以外を仮定する（一般線形モデル = 正規分布を仮定）

  - 世の中には、正規分布で表現するのが難しい事象がある
  
    - テストへの合格/不合格
    
    - 所得の分布
    
    - 信頼できる友達の数

- ```glm()```関数で使用できる分布

  - 外国語研究、言語研究では、正規分布、二項分布、対数正規分布などが用いられる

```{markdown}
binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
```

::: infobox
使用するRの関数によって、使用できる確率分布が異なります。```glm```関数には対数正規分布は含まれていません。
:::

## 二項分布
- ある試行を *N* 回行った際の成功回数 *k* が発生する確率 *q*

\[
p(k \mid N, q) = \binom{N}{k} q^k (1 - q)^{N - k}
\]

\[
\binom{N}{k} = \frac{N!}{k!(N-k)!}
\]

### 身近な例

  - 寺井と雅人がじゃんけんを10回する。寺井が雅人に *K* 回勝つ確率を求める（勝つ可能性は50 %）

  - 寺井が2回勝つ場合 (\( k = 2 \))

\[
P(X = 2) = \binom{10}{2} (0.5)^2 (0.5)^{10-2} = \binom{10}{2} (0.5)^{10}
\]

ここで、二項係数は次のように計算（8!をまとめて消している）：

\[
\binom{10}{2} = \frac{10 \times 9}{2 \times 1} = 45
\]

したがって、確率は：

\[
P(X = 2) = 45 \times (0.5)^{10} = 45 \times \frac{1}{1024} \approx 0.043945
\]

2回勝つ確率は約 **0.0439** 

  - 寺井が4回勝つ場合 (\( k = 4 \))

\[
P(X = 4) = \binom{10}{4} (0.5)^4 (0.5)^{10-4} = \binom{10}{4} (0.5)^{10}
\]


\[
\binom{10}{4} = \frac{10 \times 9 \times 8 \times 7}{4 \times 3 \times 2 \times 1} = 210
\]

したがって、確率は：

\[
P(X = 4) = 210 \times (0.5)^{10} = 210 \times \frac{1}{1024} \approx 0.205078
\]

  - 4回勝つ確率は約 **0.2051**

```{r}
# パラメータ
N <- 10  # 試行回数
k <- 4   # 成功回数
p <- 0.5  # 成功確率（例えば、50%の確率）

# 2項分布における確率を計算
prob <- dbinom(k, size = N, prob = p)
print(prob)
```

- 線形回帰を2値のデータに当てはめたモデルは、予測値（直線）が1以上であったり0以下であったりしている（本来は1 = 合格、0 = 不合格）

  - 予測値を0から1に収まるようにする必要がある。これを行うための関数がロジスティック関数

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
dat <- data.frame(
  Years_of_Study = runif(100, 0, 10)  # 0〜10年のランダムな学習年数
)
dat$Pass <- rbinom(100, 1, prob = plogis(0.8 * dat$Years_of_Study - 4)) # 正解率の確率的データ

# **1. そのまま線形回帰**
p1 <- ggplot(dat, aes(x = Years_of_Study, y = Pass)) +
  geom_point(alpha = 0.6) +  
  geom_smooth(method = "lm", se = FALSE, color = "red") +  
  labs(title = "1) 二値データに線形回帰",
       x = "Years of Study", y = "Pass (0 or 1)") +
  ylim(0,1)

p1
```

- 切片の値はマイナスとなっており、0-1という確率の範囲を超えている。
```{r}
res <- lm(Pass ~ Years_of_Study, data = dat)
sjPlot::tab_model(res)
```



### 2項分布からロジスティック回帰モデルへ
- 二項分布は成功回数の確率分布を表す。この成功回数を予測するモデルが**ロジスティック回帰モデル**

## ロジット関数
- 0か1しかとらない従属変数が1になる確率（確率は0と1ではなく、0から1の間をとる）を *P* とすると、0になる確率は 1 - *P*。この二つの比を**オッズ比**という。


\[
\text{odds ratio} = \frac{P}{1 - P}
\]

- 確率（ *P* ）を対数オッズに変換（ロジット変換という）
  
  - 対数オッズのことをロジットという
  
  - 0から1しかとらない値を、-∞ ~ ∞を取る連続値のデータに変換する関数がロジット関数

\[
\text{logit}(P) = \log \left( \frac{P}{1 - P} \right)
\]

- ロジット関数により、

  - この変換で、従属変数の値が0か1かという制約がなくなり、線形モデルとして偏回帰係数を推定できる
    
    - 線形モデルで当てはめる方が計算などの都合がいいから

- **線**形モデルとは異なり、**曲線**であるため、1単位の変化量が異なる

  - logit(0.5) = 0、logit(0.6) = 0.4 => ロジットスケールでの0.4の変換は変換前の単位での50%から60%の変更に対応
  
  - logit(0.9) = 2.2、logit(0.93) = 2.6 => ロジットスケールでの0.4の変換は変換前の単位での90%から93%の変更に対応

```{r, echo=FALSE, warning=FALSE}
# 0から1の値を等間隔で生成（ただし0と1は除く）
p_values <- seq(0.001, 0.999, length.out = 100)

# ロジット変換
logit_values <- log(p_values / (1 - p_values))

# データフレーム作成
logit_data <- data.frame(
  Before_Logit = p_values,  # 変換前の値 (0〜1)
  After_Logit = logit_values # 変換後の値
)

# グラフ描画
ggplot(logit_data, aes(x = Before_Logit, y = After_Logit)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "ロジット変換の可視化",
       x = "ロジット変換前 (0から1の値)",
       y = "ロジット変換後")
```

## ロジスティック関数
- この関数を使うことで、説明変数の集まり（線形予測子：線形結合した説明変数）がどの範囲に合っても、0 ~ 1の範囲に収まる（確率を表すのに最適！）。

  - ロジスティック回帰ではリンク関数にロジット関数をおき、確率を線形予測子に変換

  - ロジスティック関数は逆リンク関数として使われ、線形予測子から確率を復元する
  
    - リンク関数：従属変数を変換し、独立変数関数につなげる変換関数、独立変数を変換する場合は「逆」リンク関数


\[
\text{logistic}(x) = \frac{1}{1 + e^{-x}}
\]


```{r, echo=FALSE}
# ロジスティック関数の定義
logistic <- function(x) {
  1 / (1 + exp(-x))
}

# x の範囲を設定し、対応する y を計算
df <- data.frame(x = seq(-6, 6, length.out = 100))
df$y <- logistic(df$x)

# ggplot で描画
ggplot(df, aes(x = x, y = y)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "ロジスティック関数",
       x = "x",
       y = "σ(x) = 1 / (1 + exp(-x))")

```


### つまり
1. データを分析しやすいようにロジット変換をして線形回帰を行う。

2. 値をもとに戻さないと理解しずらいため、ロジスティック関数を使って0 ~ 1の範囲に戻している

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# **2. ロジット変換**
dat <- dat %>% 
  mutate(Logit_Pass = log((Pass + 0.01) / (1 - Pass + 0.01)))  # ロジット変換

# **ロジット変換後の線形回帰**
p2 <- ggplot(dat, aes(x = Years_of_Study, y = Logit_Pass)) +
  geom_point(alpha = 0.6) +  
  geom_smooth(method = "lm", se = FALSE, color = "green") +  
  labs(title = "2) ロジット変換後に線形回帰",
       x = "Years of Study", y = "Logit(Pass)")

# **3. ロジスティック回帰（元の確率に戻す）**
p3 <- ggplot(dat, aes(x = Years_of_Study, y = Pass)) +
  geom_point(alpha = 0.6) +  
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +  
  labs(title = "3) ロジスティック回帰（曲線）",
       x = "Years of Study", y = "Probability of Passing (0 to 1)")

gridExtra::grid.arrange(p1, p2, ncol = 2)
gridExtra::grid.arrange(p2, p3, ncol = 2)
```


::: infobox
一般「化」線形モデルでは、どの確率分布を使うかだけでなく、どのような変換関数で変換をするかも把握する必要がある。そのため、何でもかんでも正規分布で分析したり、データを無理やり変換し正規分布に近づけるような方法ではなく、得られたデータをそのままに、モデルの工夫でフィッティングを調整するという考え方が身につく。
:::

### 一般化線形モデルの使用上の注意

- 一般線形モデルのように、回帰係数をそのまま解釈できない

  - 変換係数を経由して、説明変数の変化が影響を受けている
  
    - 回帰分析：ある独立変数Aが1単位変化すると、従属変数はBだけ変化
    
    - ロジスティック回帰：ある独立変数Aが1単位変化すると、従属変数はexp(B)だけ変化

- ロジスティック関数はロジット関数の逆関数のこと

  - 逆関数：戻してあげる関数のこと
  
    - e.g., log(3) -> exp(log(3)) = 3
    
  - リンク関数としてロジット関数を使っているため、ロジット関数の逆関数であるロジスティック関数で戻してあげている。
  

## ハンズオンセッション
### 疑似データの用意
- Pass： テストの合否。1なら合格、0なら不合格

- Method：指導法A、指導法B

- Starting_age：英語を勉強し始めた年齢

```{r}
set.seed(123)  # 再現性のため

dat <- data.frame(
  Pass = sample(0:1, 40, replace = TRUE),  # 1 または 0
  Method = sample(c("A", "B"), 40, replace = TRUE),  # A または B
  Starting_age = sample(0:10, 40, replace = TRUE)  # 0 ~ 10 の数値
)
```

```{r}
head(dat, head = 3)
```

```{r}
summary(dat)
```

### モデルの推定
- ```glm()```関数を使用する

  - ```family```で確率分布を指定する

```{r}
library(stats)

res <- glm(
  Pass ~ Starting_age,
  family = binomial(link = "logit"),
  data = dat
)
```

- Estimate: 偏回帰係数

- Std. Error (Standard Error): 標準誤差

  - かなり重要。どれくらい推定に誤差があるかを示す指標。「同じ調査法で同じ数のデータをとりなおしてみると、推定値も結構変わるので、そのバラツキ度合い」

- *z* value：z値と呼ばれる統計量（Wald統計量とも言われる）。Estimate ÷ SE で算出。この値で、Wald信頼区間を算出し、その値がゼロから十分に離れているかの目安になる。

- Pr(>|z|): 平均が z値の絶対値で、標準偏差が1の正規分布において、マイナス無限大からゼロまでの値をとる確率。この確率が大きいほどZ値がゼロに近くなり、Estimateがゼロに近い。

- *p* 値（アスタリスク）：95%CIに0を含む場合有意とされる

```{r}
summary(res)
```

- 95%信頼区間の算出
```{r, message=FALSE}
stats::confint(res)
```

### 作図

```{r}
# 年齢（Years_of_Study）の範囲を設定
years_range <- seq(0, 10, length.out = 100)

# 各年数に対する予測確率を計算
pred_probs <- predict(res, newdata = data.frame(Starting_age = years_range), type = "response")

# 予測確率を描画
plot(
  years_range, pred_probs, 
  type = "l",  # 線で描画
  col = "blue", 
  lwd = 2, 
  xlab = "Starting Age", 
  ylab = "Pr (Pass)", 
  main = "Logistic Regression: Probability of Passing",
  ylim = c(0, 1),
  xaxt = "n", yaxs = "i"
)

# x軸のカスタムラベルを追加
axis(1, at = seq(0, 10, by = 1), labels = seq(0, 10, by = 1))
```

### オッズ比の算出
- オッズ比：オッズの変化量。ロジスティック回帰モデルの回帰係数に指数関数を適用すると算出できる。

  - 指数関数をとると、その値は必ず正の値になる
  
```{r, echo=FALSE}
# データ作成
data <- data.frame(
  `Estimate` = c("正（> 0）", "0", "負（< 0）"),
  `オッズ比` = c("> 1", "= 1", "< 1"),
  `解釈` = c("オッズが 増加",
             "オッズは 変化なし",
             "オッズが 減少")
)

kable(data, booktabs = TRUE, escape = FALSE) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

- 係数がマイナスだったので、 オッズ比 < 1となっている点に注意
```{r}
exp(res$coefficients)
```

```{r, message=FALSE}
exp(stats::confint(res))
```

### 結果報告の例と解釈
- 英語の学習年数の主効果の係数は有意ではなかった（*Estimate* = -0.21 [-0.47, 0.03], *SE* = 0.12, *z* = -1.67, *p* = .10, *OR* = 0.81 [0.63, 1.03]）。傾向として、英語学習開始歴が1年増えると、テストに合格する**オッズ**が0.81倍になる（ = 合格するオッズが [1 - 0.81 = 19] 19%低い）と予測される。

- もし係数が0.21だった場合、オッズ比は```exp(0.21) = 1.23```。この場合、

  - 英語学習歴が1年増えると、テストに合格する**オッズが1.23倍になる。つまり、テストに合格するオッズは（不合格となる場合に比べ）（1.23 - 1 = 0.23）23%増加すると予測される。

- しかし、オッズやオッズ比で結果を言われても解釈が少し難しい。```predict```関数で具体的な合格する確率を算出する方が分かりやすい。

```{r}
# 年齢（Years_of_Study）の範囲を設定
years_range <- seq(0, 10)

# 各年数に対する予測確率を計算
pred_probs <- predict(res, newdata = data.frame(Starting_age = years_range), type = "response")
```

- 100を書けて%単位に変換
```{r}
pred_probs * 100
```

- 作図
```{r}
plot(pred_probs * 100)
```


### 新たなデータで予測してみる
- 25歳で初めて場合を調べてみる
- ```type = "link"```にすると、係数が得られる
```{r}
new <- data.frame(Starting_age = 25)
pred_probs <- predict(res, newdata = new, type = "link")
```
- オッズ比を計算
```{r}
exp(pred_probs)
```

- ```type = "response"```にすると、予測確率が返される
```{r}
pred_probs <- predict(res, newdata = new, type = "response")
```

- 25歳ではじめると、1.13 %の合格確率となる
```{r}
pred_probs * 100
```


## 質的変数の場合

```{r}
means <- aggregate(Pass ~ Method, data = dat, FUN = mean)
means
```


```{r}
table(dat$Method)
```

```{r}
# Methodごとの正答率を計算
table_data <- table(dat$Method, dat$Pass)

# 棒グラフで可視化
barplot(prop.table(table_data, margin = 1), beside = TRUE,
        legend = rownames(table_data), xlab = "Pass (0 = Incorrect, 1 = Correct)",
        ylab = "Proportion", main = "Proportion of Passing by Method")
```

### トリートメントコントラスト

```{r}
dat$Method <- factor(dat$Method)

contrasts(dat$Method)
```

```{r}
res.2 <- glm(
  Pass ~ Method,
  family = binomial(link = "logit"),
  data = dat
)
```

- Method Bの係数：Method B - Method A
```{r}
summary(res.2)
```

- 95%信頼区間の算出
```{r}
stats::confint(res.2)
```

### オッズ比
- Methodのオッズ比は0.54だった。よってMethod BはAに比べ、合格の成功オッズが(1 - 0.54 = 0.46) 46%低い

```{r}
exp(res.2$coefficients)
```

```{r}
exp(stats::confint(res.2))
```

### 確率を計算

```{r}
# glmモデルの結果を使って予測確率を計算
pred_probs_A <- predict(res.2, newdata = data.frame(Method = "A"), type = "response")
pred_probs_B <- predict(res.2, newdata = data.frame(Method = "B"), type = "response")
```

```{r}
pred_probs_A
pred_probs_B
```


### 作図

```{r, message=FALSE,warning=FALSE}
#install.packages("arm")
library(arm)

# Method A と Method B の予測確率を描画
plot(
  c(1, 2), c(pred_probs_A, pred_probs_B), 
  pch = 16, col = "black", cex = 2, 
  xlim = c(0, 3), 
  ylim = c(0, 1), 
  xaxt = "n", xlab = "Method", ylab = "Pr (Pass)",
  xaxs = "i", yaxs = "i"
)

# 信頼区間のエラーバーを追加
arrows(1, pred_probs_A - 0.1, 1, pred_probs_A + 0.1, angle = 90, code = 3, length = 0.1, col = "black")
arrows(2, pred_probs_B - 0.1, 2, pred_probs_B + 0.1, angle = 90, code = 3, length = 0.1, col = "black")

# x軸のカスタムラベルを追加 (Method A と Method B)
axis(1, at = c(1, 2), labels = c("Method A", "Method B"))

```

## 分類にも使えます

- 機械学習などでは分類モデル（2つのカテゴリのデータ）として用いられたりします。

  - 以下では、合格の予測確率が0.6未満の場合やばい（0）、0.6以上の場合安心（1）と分類しています

```{r}
pred_probs <- predict(res, type = "response", newdata = data.frame(Starting_age = dat$Starting_age))

# 予測クラスを作成
pred_class <- ifelse(pred_probs > 0.6, 1, 0)

# 元のデータに予測確率と予測クラスを追加
new_data <- dat %>%
  mutate(pred_probs = pred_probs,
         pred_class = pred_class)

# ggplotで視覚化
ggplot(new_data, aes(x = Starting_age, y = 1, fill = factor(pred_class))) +
  geom_tile(height = 1, width = 1) +  # 矩形のサイズ調整
  scale_fill_manual(values = c("darkblue", "skyblue")) +  
  labs(x = "Starting Age", y = "", fill = "Predicted Class") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```


## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）
- ハンズオンセッションで使用したデータを基にサムコントラストで分析をしてみましょう

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで


## 参考文献
- 草薙（2017）　確率分布から見る外国語教育研究データ
- Gelman Regression and Other stories
- 馬場 RとStanではじめるベイズ統計モデリングによるデータ分析
- Rを用いた一般化線形混合モデル（GLMM）の分析手法を身につける:言語研究分野の事例をもとに
- 小杉　「言葉と数式で理解する多変量解析入門」
- https://bellcurve.jp/statistics/course/26934.html?srsltid=AfmBOorwQsuSpgEx3zQ8gVhrS2zSP50-PUvuzRlNqNP-rxWB2J_XBwyf
- https://hkawabata.github.io/technical-note/note/ML/logistic-regression.html
- Terai, M., Fukuta, J., & Tamura, Y. (2024). Learnability of L2 collocations and L1 influence on L2 collocational representations of Japanese learners of English. International Review of Applied Linguistics in Language Teaching, 62(4), 1959-1983.

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:10-Week10_logistic.Rmd-->

# Week11: 階層モデル

```{r, include=FALSE}
library(rstan)
library(brms)
library(gt)
library(tidyverse)
library(kableExtra)

rstan_options(auto_write = T)
options(mc.cores = parallel::detectCores())
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 




## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）
- ハンズオンセッションで使用したデータを基にサムコントラストで分析をしてみましょう

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:11-Week11_GLMM.Rmd-->

# Week 12: 効果量と検定力分析

```{r, include=FALSE}
library(rstan)
library(brms)
library(gt)
library(tidyverse)
library(kableExtra)

rstan_options(auto_write = T)
options(mc.cores = parallel::detectCores())
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 




## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）
- ハンズオンセッションで使用したデータを基にサムコントラストで分析をしてみましょう

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:12-Week12_Eff_pow.Rmd-->

# Week 13: ノンパラメトリック検定

```{r, include=FALSE}
library(gt)
library(tidyverse)
library(kableExtra)
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 




## 次週までの課題
### 課題内容

1. 小テストに向けて今回の内容を復習する。必ず手でコードを入力してRを実行する。

2. （宿題を考える）
- ハンズオンセッションで使用したデータを基にサムコントラストで分析をしてみましょう

- **Rで数値を出力するだけでなく、それぞれの質問への回答を高校生にもわかりやすく文字で記載してください。**

### 提出方法
- メールにファイルを添付して送信。
- 締め切りは今週の木曜日まで

## 参考文献

```{=html}
<style>
.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

<!--chapter:end:13-Week13_nonpara.Rmd-->

# Week 15：言語研究とオープンサイエンス

```{r, include=FALSE}
library(tidyverse)
library(kableExtra)
```


## 事前の確認

- この講義のRプロジェクトを開いていますか？
- 英数字で名前を付けた本日の講義のファイルを作成しましたか？

  - .Rでも.Rmdでもどちらでも大丈夫です。

## 今日の目標

1. 
2. 

```{=html}
<style>

/* Whole document */
body {
  font-family: Helvetica;
  font-size: 16pt;
}

/* Headers */
h1, h2, h3, h4, h5, h6 {
  font-size: 24pt;
}

.infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid orange;
  border-radius: 10px;
  background: #f5f5f5 5px center/3em no-repeat;

}

.beg {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHtu3kBX8P39WYBBAjar9c8c1ladK2SYL6_gEMXFweQfauWVhSvCQP5KELsPX5KNL1uOddLLQ-aeMxv904OW_NFFfANhBYObfBV09KO2EXehrb9kMdCLZY1afsChib-7zIkBJbG6OrbJpM/s400/aisatsu_kodomo_boy.png");}

.caution {
  background-image: url("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzMqkpQ7vLUKvumbm6AFwTLQiCe7tlDb2Q0MAiISLsesZHnhj0kbRjB4U3se3UrDIHfIy0hlahyphenhyphenQu-V2tOR2LcV_lX7U8P5a8jtqPYv3Ah4L-JoYi8PhoaoehumGIdp2vrsX0rRyhXqwA/s800/mark_chuui.png");}
  
</style>
```

::: {.infobox .beg data-latex="{beg}"}
2024年8月に名古屋学院大で行われた外国語メディア学会全国大会でのワークショップ（外国語教育研究者のためのオープンサイエンス入門― R Markdownを用いた実践編の内容を改変したものです。
:::


## はじめに

### 研究成果や研究資料の公開・共有の必要性

#### 再現可能な研究のために、研究に使用したマテリアルの提出を推奨

-   e.g., 実験で使用した刺激文、分析に使用した生データ、分析コード

-   [Language LearningのHPより](https://onlinelibrary.wiley.com/page/journal/14679922/homepage/ForAuthors.html#badgel)

> **Shared Research Materials and Data Policy for Accepted Articles.** <mark style="background-color: orange;">*Language Learning* encourages accepted authors to upload their data collection materials and/or data to the IRIS database (<http://www.iris-database.org>).</mark> IRIS is an online repository for data collection materials used for second language research. This includes data elicitation instruments such as <mark style="background-color: orange;">interview and observation schedules, language tests, pictures, questionnaires, software scripts, URL links, word lists, pedagogical interventions, and so on.</mark> (...) The sharing of research instrumentation benefits the research community and helps authors and journals increase the visibility of their published research.



## 1. R Markdownを使って分析結果をまとめることができる

### R Studioを開いて作成する

#### R Markdownの解体新書

-   【パーツ１】YAML（YAML　Ain't Markup Language）ヘッダー：文章全体の体裁や情報を操作する

    -   タイトル、サブタイトル
    -   作成者
    -   作成した日時、更新日時も設定可能
    -   どのような形式で作成するか

::: {.infobox .caution data-latex="{caution}"}
**注意**<br>
・ YAMLヘッダーは、RでもMarkdownでもないプログラム言語で記述します。
:::

-   【パーツ２】コードチャンク：Rのコードを記述するところ

-   【パーツ３】ドキュメントチャンク：[Markdownと呼ばれるプログラム言語](https://www.jaysong.net/RBook/rmarkdown.html#rmarkdown-intro)で記述するところ

    -   見出し、表、箇条書き、強調、斜体など、Wordのリボン部分にある機能をMarkdownで書く


### Knit🧶を押して出力！
- 初期設定はHTMLファイル出力です

## プロジェクト

-   プロジェクト = ディレクトリ
    -   ファイルや操作履歴を保存できる
-   プロジェクトを作成する利点
    -   研究ごとに分析に必要なファイルをまとめることができる
    
## 実際に作ってみましょう
- デスクトップに新しいフォルダーを作成してください
    - 名前は、英数字のみがいいです（Rが関係しそうな場合、ファイル名、フォルダ名に日本語を使わない方が安心です）
    -   作り方を解説しているサイト（[私たちのR](https://www.jaysong.net/RBook/project.html)）
-   デスクトップのフォルダーを指定してプロジェクトを作成しましょう


## ドキュメントチャンク：Markdown記法

### 覚えるのはマストではない。その都度調べてよく使うものを覚えていく

-   Markdownなら生成AIはほぼ完ぺきに正解を教えてくれる
-   [必要最低限で覚えておくとよい記法](https://qiita.com/tbpgr/items/989c6badefff69377da7)
    -   見出し → これはマスト！
        -   #の数で指定。文字との間を半角あけるのを忘れない。
    -   箇条書き
        -   `*`, `+`, `-`のいずれかを入れる。文字との間を半角あけるのを忘れない。
        -   半角スペースを2つ前（もしくはtab）に入れると、レベル２を作れる。さらに2ついれると、、、
    -   強調
        -   \*で挟むと斜体
        -   \*\*で挟むとBold体
        -   \*\*\*で挟むとどうなるでしょう

## Let's 実践

-   以下の文章をMarkdownを使って再現してください。

::: infobox
'# 名古屋飯といえば

'## ひつまぶし：*Hitsumabushi*

おすすめは以下のお店です。

- **ひつまぶし花岡**
  - 場所：栄
:::

## 答え合わせ

```{markdown}
# 名古屋飯といえば
## ひつまぶし：*Hitsumabushi*
おすすめは以下のお店です。

- **ひつまぶし花岡**
  - 場所：栄

```

## 実は、Wordのように編集できます！

-   Markdownで書かなくとも、VisualモードであればWordと似たようにできます。
-   以下で設定ができる
    -   [Tools] → [Global Options...] → [R Markdown] → [Visual]
    -   "Use visual editor by default for new documents"の項目に☑
    -   "Soft-wrap R Markdown files"にも☑を入れると、右側にアウトラインが出ます
-   欠点として、少し動作が遅い。簡単なものはMarkdownで書く方が速い
-   表などはVisualモードがおすすめ

## コードチャンクの挿入

### ショートカットキーが便利：[Ctrl] + [Alt] + [I]（Windows）、[Command] + [Option] + [I]（Mac）

-   このコードの中はRです。Rで使う関数などを自由に指定できます。
-   以下のチャンク内でないと、動きません。

\`\`\`{r}\
\`\`\`

```{r}
dat <- c(1, 4, 6)

mean(dat)

```

```{r}
plot(dat)
```

-   {r}の中にもいろいろな指定ができます。

    -   コードを非表示にして結果だけを表示させたい（オープンサイエンスではないですが）

```{r, echo=FALSE}
dat <- c(1, 4, 6)

mean(dat)
```

-   チャンクのオプションは沢山あるので、その都度チートシートを参照するとよいです。

    -   [チートシート](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

## Let's 実践

-   以下をドキュメントチャンクとコードチャンクを使って再現してください。

::: infobox
## 食費の合計

- 以下は、名古屋旅行で使った食費の合計である。

  - **注!** *hitsu*はひつまぶし、*miso*は味噌カツを表す。

```{r}
hitsu <- 1300 * 2

miso <- 1000 * 2

total <- sum(hitsu, miso)
```
:::

## 答え合わせ

```{markdown}
## 食費の合計
- 以下は、名古屋旅行で使った食費の合計である。

  - **注!** *hitsu*はひつまぶし、*miso*は味噌カツを表す

\```{r}
hitsu <- 1300 * 2

miso <- 1000 * 2

total <- sum(hitsu, miso)
\```

```

## 2. R Markdownでまとめた結果を共有することができる

-   YAMLヘッダーのoutputを変更するだけ！

### Wordに出力

### Before

```{markdown}         
title: "Untitled"
output: html_document
date: "2024-04-15"
```

### After

```{markdown}         
title: "Untitled"
output: word_document
date: "2024-04-15"
```

-   テンプレートを追加することも可能。テンプレートはきちんとレベル分けの設定などを行っておく必要あり[（設定の仕方）](https://gedevan-aleksizde.github.io/rmarkdown-cookbook/word-template.html)。

### Power Pointに出力

-   図や表は必ず新しいページに表示されるなど、knitした後の修正が面倒。

```{markdown}         
title: "Untitled"
output: powerpoint_presentation
date: "2024-04-15"
```

### HTML slideに変更

```{markdown}         
title: "Untitled"
output: ioslides_presentation
date: "2024-04-15"
```

-   **\#**で定義するレベル分けでスライドの区切りが代わる

    -   **\#**でスライドのセクション見出し
    -   **\##**で新しいページ
    -   **\###**ページ内の太文字

-   枠にとらわれない！

    -   以下のチャンクを先頭に入れると、スクロール可能なスライドになる（チャンク内はrではなく、`=html`にする）。
    -   注意点として、タッチパッドや、マウスホイールでスクロールできますが、スクロールバーを掴んでスクロールはできません。

```{markdown}         
<style>
  slides > slide {
    overflow-x: auto !important;
    overflow-y: auto !important;
}
</style>
```

## HTML形式だと、簡単にウェブサイトにできます

-   リンクで他者に共有できます。
-   以下の二つは無料で利用できるが、無料版の場合、リンクを知る人だれもが閲覧できるので注意

### RPubsを使う

-   コメント機能もあるので、「発表へのコメントは匿名でこちらへ」みたいにできそう。
-   [使い方の解説](https://qiita.com/masato-terai/items/664c5ee782f690260eca#rpubs%E4%B8%80%E7%95%AA%E7%B0%A1%E5%8D%98)

### Githubを使う

-   Githubとは、コードのバージョン管理をするツール。共同編集可能。
-   今回は一番簡単な方法で実行します。しかし、Git(hub)をフルで使いこなせば、ファイルのバージョン管理、共同編集などが可能でより再現可能な資料作成に一歩近づくと思います。R Studioとの連携も可能。
-   [使い方の解説](https://qiita.com/masato-terai/items/664c5ee782f690260eca#rpubs%E4%B8%80%E7%95%AA%E7%B0%A1%E5%8D%98)

## 3. 様々なパソコンで同じ分析環境を再現することができる

### パッケージのバージョン管理：`renv`パッケージ

-   *r*eproducible *env*ironments（再現性のある分析環境）の略

-   RのプロジェクトごとにRの環境を作る。同じパソコンで、同じ名前でバージョンが異なるパッケージを使うことができる。

-   旧バージョンを試しに使ってみたい場合が、今のバージョンも記録しておきたいときに！

-   プロジェクトディレクトリを相手に共有できる！別のパソコンで分析したいときにも！

-   Let's インストール

```{markdown}         
install.packages("renv")
```

-   使用する3つの関数

    1.  `init`関数：パッケージ管理の開始を宣言
    2.  `snapshot`関数：パッケージ情報の保存
    3.  `restore`関数：パッケージ情報の復元

### バージョン管理の開始

-   以下のコマンドを走らせると、**プロジェクト内で**使用しているパッケージを、RやRmdファイルなどから検出します。そして、そのバージョンの情報を`renv.lock`ファイルに保存します。

```{markdown}         
renv::init()
```

-   最初に実行すると、以下のメッセージが表示されます。

```         
renv: Project Environments for R

Welcome to renv! It looks like this is your first time using renv.
This is a one-time message, briefly describing some of renv's functionality.

renv will write to files within the active project folder, including:

  - A folder 'renv' in the project directory, and
  - A lockfile called 'renv.lock' in the project directory.

In particular, projects using renv will normally use a private, per-project
R library, in which new packages will be installed. This project library is
isolated from other R libraries on your system.

In addition, renv will update files within your project directory, including:

  - .gitignore
  - .Rbuildignore
  - .Rprofile

Finally, renv maintains a local cache of data on the filesystem, located at:

  - "C:/Users/terai-masato/AppData/Local/R/cache/R/renv"

This path can be customized: please see the documentation in `?renv::paths`.

Please read the introduction vignette with `vignette("renv")` for more information.
You can browse the package documentation online at https://rstudio.github.io/renv/.
Do you want to proceed? [y/N]: 
```

-   yを押して進むと、開いているプロジェクトのあるディレクトリに**renv**というフォルダーが作成されます。その中に、3つのファイルと、1つのディレクトリが作成されています。また、**renv.lock**というファイルも同じディレクトリに作成されます。**新しく作成されたものは、すべて`renv`パッケージの利用に必要なので、削除しないでください。**

```         
- "C:/Users/terai-masato/AppData/Local/R/cache/R/renv" has been created.
- Linking packages into the project library ... [33/33] Done!
- Resolving missing dependencies ... 
# Installing packages --------------------------------------------------------
The following package(s) will be updated in the lockfile:

# CRAN -----------------------------------------------------------------------
- base64enc     [* -> 0.1-3]
- bslib         [* -> 0.5.1]
- cachem        [* -> 1.0.8]
- cli           [* -> 3.6.1]
- digest        [* -> 0.6.33]
- ellipsis      [* -> 0.3.2]
- evaluate      [* -> 0.23]
- fastmap       [* -> 1.1.1]
- fontawesome   [* -> 0.5.2]
- fs            [* -> 1.6.3]
- glue          [* -> 1.6.2]
- highr         [* -> 0.10]
- htmltools     [* -> 0.5.7]
- jquerylib     [* -> 0.1.4]
- jsonlite      [* -> 1.8.7]
- knitr         [* -> 1.45]
- lifecycle     [* -> 1.0.4]
- magrittr      [* -> 2.0.3]
- memoise       [* -> 2.0.1]
- mime          [* -> 0.12]
- prettydoc     [* -> 0.4.1]
- R6            [* -> 2.5.1]
- rappdirs      [* -> 0.3.3]
- renv          [* -> 1.0.7]
- rlang         [* -> 1.1.2]
- rmarkdown     [* -> 2.25]
- sass          [* -> 0.4.7]
- stringi       [* -> 1.7.12]
- stringr       [* -> 1.5.0]
- tictoc        [* -> 1.2]
- tinytex       [* -> 0.48]
- vctrs         [* -> 0.6.4]
- xfun          [* -> 0.41]
- yaml          [* -> 2.3.7]

The version of R recorded in the lockfile will be updated:
- R             [* -> 4.3.2]

- Lockfile written to "~/LET/LET_Workshop_2024/materials_LETworkshop_2024/renv.lock".

Restarting R session...

- Project '~/LET/LET_Workshop_2024/materials_LETworkshop_2024' loaded. [renv 1.0.7]
```

### パッケージ情報の保存

-   パッケージの追加・更新・削除を行ったら、`snapshot`関数を使って**renv.lock**ファイルを更新します。これを忘れると、バージョン情報が変更されないので、注意。

-   試しに、新しいパッケージをインストールしましょう。今回は、Rに関する様々な名言を含んでいる`fortunes`パッケージをインストールしましょう。興味のある方はこちらにリストがあります([R Fortunes: Collected Wisdom](https://cran.r-project.org/web/packages/fortunes/vignettes/fortunes.pdf))。

```{markdown}         
install.packages("fortunes")
```

```{r}
packageVersion("fortunes")
```


```{r}
fortunes::fortune(which = 78)
```

```{r}
fortunes::fortune(which = 386)
```

-   今回の追加をrenv.lockファイルに追加しましょう。

```{markdown}         
renv::snapshot()
```

```         
The following package(s) will be updated in the lockfile:

# CRAN -----------------------------------------------------------------------
- fortunes   [* -> 1.5-4]

Do you want to proceed? [Y/n]: 
```

-   ファイルが更新される。

```         
- Lockfile written to "~/LET/LET_Workshop_2024/materials_LETworkshop_2024/renv.lock".
```
-   renvフォルダーの中の、library \> R-あなたのバージョン \> あなたのパソコンPlatformを開くと、パッケージの名前と同じフォルダ（**fortunes**）が追加されています（五十音順になっているので見つけやすいです）。

### パッケージ情報の復元

-   記録しておくことで、１）パッケージを前のバージョンに戻したり、２）他のパソコンにプロジェクトの分析環境を整えたりすることができる。

-   パッケージの更新では、お馴染みの`install.packages`関数や、`update.packages`関数、`remove.packages`関数を使うが、`renv`関数で管理しているプロジェクト内では、これらの関数は`renv`関数内のパッケージを呼び出している。特に、バージョンを指定したパッケージのインストールがしやすくなっている。

-   以下のように、[パッケージ名\@バージョン名で指定](mailto:パッケージ名@バージョン名で指定){.email}

    -   今回は、`fortunes`パッケージの古いバージョンをインストールする。

-   最初に、`remove.packages`で**fortunes**パッケージを削除。\

-   注！ここで`renv::snapshot()`はやらない。やってしまうと、renv.lockからも削除され、戻らなくなってしまう。

```{r, eval=FALSE}
install.packages("fortunes@1.4-0")
```

```         
# Installing packages --------------------------------------------------------
- Installing fortunes ...                       OK [linked from cache]
Successfully installed 1 package in 22 milliseconds.

The following loaded package(s) have been updated:
- fortunes
Restart your R session to use the new versions.
```

-   restart（quit session）をして再度開く。
-   **renv.lock**ファイルは確認すると、1.5-4のまま！
-   しかし、パッケージのバージョンは1.4.0!

```{r, eval=FALSE}
packageVersion("fortunes")
```

-   [バージョン1.4.0は2010年9月9日にリリースされている](https://cran.r-project.org/src/contrib/Archive/fortunes/)
-   2016年に追加された386番目の名言はこのバージョンでは追加されていないので表示されない。

```{r, eval=FALSE}
fortunes::fortune(which = 78)
```

```{r, eval=FALSE}
fortunes::fortune(which = 386)
```

-   元に戻す場合は、`renv::restore()`

```{markdown}         
renv::restore()
```

```         
The following package(s) will be updated:

# CRAN -----------------------------------------------------------------------
- fortunes   [1.4-0 -> 1.5-4]

Do you want to proceed? [Y/n]: 
```

<!-- - 再度quit sessionをすると、 -->

<!-- ```{r} -->

<!-- packageVersion("fortunes") -->

<!-- ``` -->

::: {.infobox .caution data-latex="{caution}"}
**注意**<br>
・ パッケージによっては、手動でのダウンロードが必要なものがあります。その場合、エラーメッセージでどのパッケージのインストールができないのか表示されます。エラーメッセージに表示されているパッケージを見て、手動でそれをインストールします。
:::

## R自体のバージョン管理も可能です

### Windows

-   [Tools] → [Global Options] → [R version]をクリックすると、過去にそのPCで使用していたRのバージョンが記録されている。

### Mac

-   Windowsのようなセレクトボタンがない！
-   [RSwitch](https://rud.is/rswitch/)をダウンロードする必要あり

## おまけ

### R Markdownに含めておきたい情報
### 日付、日時
- いつ作成されたファイルで、更新はされたことあるのかを明記することが重要
    -   デフォルトでは、ファイルの作成日を手動で切り替えないといけない。

-   r Sys.Date：2022-01-20

```{markdown}         
title: "Untitled"
author: "Masato Terai"
date: "`r Sys.Date()` in JST"   # ``と""で囲うのを忘れない。
output: html_document
```

-   r Sys.time：2022-01-20 21:36:36

```{markdown}         
title: "Untitled"
author: "Masato Terai"
date: "`r format(Sys.time(), '%Y-%m-%d %X')`"   # ``と""で囲うのを忘れない。
output: html_document
```

- 寺井おすすめ
```{markdown}         
title: "Untitled"
author: "Masato Terai"
date: "作成日:2024-05-20, 最終更新(JST): `r format(Sys.time(), '%Y-%m-%d %X')`"
output: html_document
```


### Rのバージョン & 使用したRのパッケージのバージョン
- Rのバージョンやパッケージのバージョン情報など

```{r, warning=FALSE}
sessionInfo()
```

### パソコンのスペック
- CPUとコア数とRAM
  - CPU
  - コア数（論理コア）
    - 「論理コア」は「スレッド」「論理プロセッサ」、「仮想コア」とも呼ばれる
    - [CPUとコアについて](https://wa3.i-3-i.info/diff735core.html)
  - RAM (Random Access Memory)
    - ストレージ（SSDやHDD）ではなく、データを一時保存する場所
  
```{r}
#install.packages("benchmarkme") # 入れていないかたは先にインストールしてください
benchmarkme::get_cpu()
```

```{r}
benchmarkme::get_ram()
```


### 処理にかかった時間

- 時間のかかる分析を行う人は明記する方が親切！
  - いつ終わるのか分からない見通しのつかない分析を実行するのは怖いです。コア数に加え処理にかかった時間が記載されていればおおよその見通しがつきます。

```{r}
#install.packages("tictoc") # 入れていないかたは先にインストールしてください
library(tictoc)

tic() #測定開始

I <-NULL
for(i in 1:100000){
  I <- c(I,  i)}

toc() #測定終了
```

## 関数の呼び出し方

1.  `library(関数名)`：一番メジャー

2.  `パッケージ名::関数名`：あまりメジャーじゃない

3.  `require(関数名)`：`library()`とほとんど変わらないが、関数を読み込めたかどうかを論理値で返すことができる。関数が読み込めない（＝パッケージを入れていない）場合に、まずそのパッケージを読み込んで、処理に進ませるみたいな用途で使える。

```{markdown}
if (require(パッケージ名) == FALSE) {
 install.packages(パッケージ名)
} else {
 #パッケージを利用してやろうとしていたことをここに書く
}
```

### 関数名が被ることがある（xxxはマスクされています）

-   グラフの透明度を設定する`alpha`関数（`ggplot2`パッケージ）と、信頼性係数を出す`alpha`関数（`psych`パッケージ）

    -   エラーの原因になる場合あり。`::`で定義すると回避できる。

```{markdown masa, include = F}         
> library(ggplot2)
Need help getting started? Try the R Graphics
Cookbook: https://r-graphics.org

 次のパッケージを付け加えます: ‘ggplot2’ 

 以下のオブジェクトは ‘package:psych’ からマスクされています:

    %+%, alpha
```


```{r, echo=FALSE, warning = F}
library(flair)
decorate("masa") %>% 
  flair("以下のオブジェクトは ‘package:psych’ からマスクされています:", background = "#FDB933") %>%
  flair("%+%, alpha", background = "#FDB933")
```

### 関数とパッケージの対応は覚えにくい

-   R Markdownのファイルの冒頭にこのようにまとめてある。確かに、その分析で使うパッケージが一目瞭然だが、個別の処理においてどのパッケージが読み込まれたのか分かりにくく、そのコードを参考に書き換えにくくなる。

```{markdown ex, include =F}         
library(dplyr)
library(stats)
```


```{r, echo=FALSE, warning = F}
library(flair)
decorate("ex") %>% 
  flair("dplyr", background = "#FDB933") %>%
  flair("stats", background = "#FDB933")
```

-   どのパッケージに属する関数なのか一目瞭然。ただしその都度パッケージ名を書くので、コードが長くなるという欠点も

```{markdown example, include = F}         
data |>
  dplyr::select() |>
  stats::lm(formula = cyl ~ am)
```

```{r, echo=FALSE, warning = F}
library(flair)
decorate("example") %>% 
  flair("dplyr::", background = "#FDB933") %>%
  flair("stats::", background = "#FDB933")
```


## まとめ
<div style="border: 3px solid blue; padding: 10px; background-color: #e6f7ff;">
1.  R Markdownでは、YAMLヘッダー、コードチャンク、ドキュメントチャンクを使って分析結果をまとめることができる。

    -   研究ごとにプロジェクトを作成しておくとよい

2.  作成したR Markdownファイルは、Word、HTMLファイル、Power Pointなど様々な形式に出力できる。

    -   RPubsやGithubを使えば、ウェブページとして共有できる

3.  RのパッケージやR自体のバージョンは切り替えて使うことができる。

    -   再現しようとする分析ファイルに記載されたバージョンに合わせて分析することでより再現性が高まる
</div>

<br>

## 参考文献 & 資料

### 📚：書籍、💻：ウェブサイトの記事

-   [💻「物理コア」と「論理コア」の違い](https://wa3.i-3-i.info/diff735core.html)

-   [💻MacでのRstudio上でのR versionの切り替え方法](https://qiita.com/gp333/items/9ed91fc280fc6f26ef93)

-   [📚Rが生産性を高める データ分析ワークフロー効率化の実践](https://www.amazon.co.jp/R%E3%81%8C%E7%94%9F%E7%94%A3%E6%80%A7%E3%82%92%E9%AB%98%E3%82%81%E3%82%8B%E3%80%9C%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC%E5%8A%B9%E7%8E%87%E5%8C%96%E3%81%AE%E5%AE%9F%E8%B7%B5-igjit/dp/4297125242)

-   [💻Rで関数の前に::をつけるのなんで？](https://beginner-r.com/why_two_colons_are_needed_in_rlang/)

-   [💻R言語でパッケージから関数を呼び出す](https://webbeginner.hatenablog.com/entry/2015/07/23/064441)

-   [💻Switching R versions in Windows](https://bioinformatics.ccr.cancer.gov/docs/rtools/R%20and%20RStudio/2.6_switching_r_version/)

-   [📚再現可能性のすすめ：RStudioによるデータ解析とレポート作成](https://www.amazon.co.jp/%E5%86%8D%E7%8F%BE%E5%8F%AF%E8%83%BD%E6%80%A7%E3%81%AE%E3%81%99%E3%82%9D%E3%82%81-Wonderful-R-%E7%9F%B3%E7%94%B0-%E5%9F%BA%E5%BA%83/dp/4320112431)

-   [💻私たちのR](https://www.jaysong.net/RBook/renv.html)

-   [💻YAMLについての基本知識](https://qiita.com/xuwenzhen/items/b01f78525626b3e87d23)

-   [💻R Markdownでスライドを作成（ioslides）](https://qiita.com/masato-terai/items/664c5ee782f690260eca)

-   [💻R Markdown クックブック](https://gedevan-aleksizde.github.io/rmarkdown-cookbook/)

-   [💻R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

<!--chapter:end:15-Week15_Repro.Rmd-->

# Sharing your book

以下の説明はダメ
標本サイズ30以上ならば大丈夫
正規性検定で正規性の帰無仮説が棄却されなかったら、正規分布モデルを使って良い

＊母集団分布の形状について何も情報がないならば、必要な標本サイズの上限は決まらなくなります。
＊身長なら正規分布モデルに従うことが和kる
＊母平均があなたの目的にとって適切な代表値なのかどうかをよく考えよ。
＊検定達は、確かに正規分布モデルを使って導出される検定法ですが、中心極限定理のお陰で、正規分布モデル自体が誤りになるような未知の母集団分布の場合であっても許容できる誤差の範囲内で使用可能な場合が結構沢山あります。(もちろんダメな場合もある。)
＊正規分布でないからノンパラ使おう（無条件では）はだめ
＊Mann-WhitneyのU検定のP値は

2つの母集団分布はぴったり等しい

という仮定を使って計算されるので、2群の優劣のために使用するのは危険な場合があります。

無条件では使えない使い方が難しい検定法なので注意が必要。Mann-WhitneyのU検定を使っている報告は粗探しをする必要があります。
T検定はウェルチをデフォルトで使う
＊例えば、ある特定目的のための2つの集団の比較で母平均の差の推定値を使うことが適切であるか、という問題は、その目的と2つの集団とデータの取得法に関する専門知識がないと手も足も出ない問題になります。統計学の使用では

Science before Statistics! 

という合言葉が必要。

*Use of nomality tests before t test
https://discourse.datamethods.org/t/reference-collection-to-push-back-against-common-statistical-myths/1787#use-of-normality-tests-before-t-tests-13

フローチャートに従って決めるなどは推奨されない。
→研究したいこと、見たいことは何かを考える
→研究分野に対する知識が重要。〇→XならAIにもできるし、言語研究の専門家でなくても分析ができてしまう。→自分の研究したいことは自分が一番よく理解しようという心構えが自分の分野の将来も守る
過去の研究や理論的背景から、母集団分布の形を考える

引用：https://biostatistics.ucdavis.edu/sites/g/files/dgvnsk4966/files/media/documents/Greenland.Advancing%20statistics%20reform%2C%20part%204.Slides%201-110%2C%2001%20June%202022.pdf

<!--chapter:end:share.Rmd-->

